{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import json\n",
    "import geojson\n",
    "import requests\n",
    "import base64\n",
    "import sqlite3 as db\n",
    "import cgi\n",
    "sys.path.append('../')\n",
    "\n",
    "pd.options.display.max_rows=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_userName = 'Tanag3r'\n",
    "ebird_token = 'j6c7l80ga2ib'\n",
    "cnx = db.connect('trailheadDirectBirds_sous.db')\n",
    "cur = cnx.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain & clean trailhead data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Route</th>\n",
       "      <th>StopName</th>\n",
       "      <th>Address</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>EastSunsetWay</td>\n",
       "      <td>661-831 E Sunset Way, Issaquah, WA 98027</td>\n",
       "      <td>47.529635</td>\n",
       "      <td>-122.025119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>HighSchool</td>\n",
       "      <td>Parking lot, The Rainier Trail, Issaquah, WA 9...</td>\n",
       "      <td>47.519345</td>\n",
       "      <td>-122.029801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>ChiricoTrail_PooPooPoint</td>\n",
       "      <td>11400 Issaquah-Hobart Road Southeast, Issaquah...</td>\n",
       "      <td>47.499949</td>\n",
       "      <td>-122.02173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>SquakMountain</td>\n",
       "      <td>13201 Squak Mountain Rd SE, Issaquah, WA 98027</td>\n",
       "      <td>47.481465</td>\n",
       "      <td>-122.053997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>NullMountain</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>MargaretsWay</td>\n",
       "      <td>190th Ave SE, Issaquah, WA 98027</td>\n",
       "      <td>47.50662</td>\n",
       "      <td>-122.08666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MountSi</td>\n",
       "      <td>MountSi</td>\n",
       "      <td></td>\n",
       "      <td>47.488966</td>\n",
       "      <td>-122.723044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MountSi</td>\n",
       "      <td>MountTeneriffe</td>\n",
       "      <td>Mount Teneriffe Rd, North Bend, WA 98045</td>\n",
       "      <td>47.490104</td>\n",
       "      <td>-122.709182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MountSi</td>\n",
       "      <td>LittleSi</td>\n",
       "      <td>SE Mt Si Rd, North Bend, WA 98045</td>\n",
       "      <td>47.489366</td>\n",
       "      <td>-122.753833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SeattleParks</td>\n",
       "      <td>DiscoveryParkNorth</td>\n",
       "      <td>Discovery Park Road &amp; Texas Way, Seattle, WA 9...</td>\n",
       "      <td>47.664631</td>\n",
       "      <td>-122.411063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SeattleParks</td>\n",
       "      <td>DiscoveryParkBeach</td>\n",
       "      <td>Fort Lawton Beach, Seattle, WA 98199</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SeattleParks</td>\n",
       "      <td>DiscoveryParkSouth</td>\n",
       "      <td>W Emerson St &amp; Magnolia Blvd W, Seattle, WA 98199</td>\n",
       "      <td>47.654152</td>\n",
       "      <td>-122.412975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Route                  StopName  \\\n",
       "index                                           \n",
       "0      IssaquahAlps             EastSunsetWay   \n",
       "1      IssaquahAlps                HighSchool   \n",
       "2      IssaquahAlps  ChiricoTrail_PooPooPoint   \n",
       "3      IssaquahAlps             SquakMountain   \n",
       "4      IssaquahAlps              NullMountain   \n",
       "5      IssaquahAlps              MargaretsWay   \n",
       "6           MountSi                   MountSi   \n",
       "7           MountSi            MountTeneriffe   \n",
       "8           MountSi                  LittleSi   \n",
       "9      SeattleParks        DiscoveryParkNorth   \n",
       "10     SeattleParks        DiscoveryParkBeach   \n",
       "11     SeattleParks        DiscoveryParkSouth   \n",
       "\n",
       "                                                 Address   Latitude  \\\n",
       "index                                                                 \n",
       "0               661-831 E Sunset Way, Issaquah, WA 98027  47.529635   \n",
       "1      Parking lot, The Rainier Trail, Issaquah, WA 9...  47.519345   \n",
       "2      11400 Issaquah-Hobart Road Southeast, Issaquah...  47.499949   \n",
       "3         13201 Squak Mountain Rd SE, Issaquah, WA 98027  47.481465   \n",
       "4                                                   <NA>       <NA>   \n",
       "5                       190th Ave SE, Issaquah, WA 98027   47.50662   \n",
       "6                                                         47.488966   \n",
       "7               Mount Teneriffe Rd, North Bend, WA 98045  47.490104   \n",
       "8                      SE Mt Si Rd, North Bend, WA 98045  47.489366   \n",
       "9      Discovery Park Road & Texas Way, Seattle, WA 9...  47.664631   \n",
       "10                  Fort Lawton Beach, Seattle, WA 98199       <NA>   \n",
       "11     W Emerson St & Magnolia Blvd W, Seattle, WA 98199  47.654152   \n",
       "\n",
       "        Longitude  \n",
       "index              \n",
       "0     -122.025119  \n",
       "1     -122.029801  \n",
       "2      -122.02173  \n",
       "3     -122.053997  \n",
       "4            <NA>  \n",
       "5      -122.08666  \n",
       "6     -122.723044  \n",
       "7     -122.709182  \n",
       "8     -122.753833  \n",
       "9     -122.411063  \n",
       "10           <NA>  \n",
       "11    -122.412975  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pull in trailhead stops from trailheadDirectBirds_sous db\n",
    "trailheadRef = pd.DataFrame()\n",
    "trailheadRef = pd.read_sql('select * from trailheadRef;',cnx)\n",
    "trailheadRef.set_index('index',inplace=True)\n",
    "trailheadRef.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukew\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "##For the sake of demonstration please assume the data in the 'trailheadRef' comes from a King County Metro service\n",
    "##and/or is user-provided.\n",
    "\n",
    "##set types for error checking, other cleaning\n",
    "trailheadRef['Latitude'] = trailheadRef['Latitude'].fillna(0.00)\n",
    "trailheadRef['Longitude'] = trailheadRef['Longitude'].fillna(0.00)\n",
    "trailheadRef['Latitude'] = trailheadRef['Latitude'].astype('float64',errors='raise')\n",
    "trailheadRef['Longitude'] = trailheadRef['Longitude'].astype('float64',errors='raise')\n",
    "trailheadRef['CoordinateTest'] = ''\n",
    "trailheadRef['CoordinateTest'] = trailheadRef['CoordinateTest'].astype('str')\n",
    "trailheadRef['AddressTest'] = ''\n",
    "trailheadRef['AddressTest'] = trailheadRef['AddressTest'].astype('str')\n",
    "trailheadRef['failCode'] = ''\n",
    "trailheadRef['failCode'] = trailheadRef['failCode'].astype('str')\n",
    "\n",
    "##todo #10 finish this evaluator engine\n",
    "\n",
    "##warnings:\n",
    "trailheadRef.loc[(abs(trailheadRef['Latitude']) < 1) | (abs(trailheadRef['Longitude']) < 1), 'CoordinateTest'] = 'GEO10'\n",
    "trailheadRef.loc[(trailheadRef['Address'].isna()) | (trailheadRef['Address'].isnull()) | (trailheadRef['Address'] == ''), 'AddressTest'] = 'GEO20'\n",
    "##failures:\n",
    "trailheadRef.loc[(trailheadRef['CoordinateTest'] != '') & (trailheadRef['AddressTest'] != ''), 'failCode'] = 'GEO30'\n",
    "##update the test below when the coordinate fetch engine is complete to reflect failure of fetch\n",
    "trailheadRef.loc[(trailheadRef['CoordinateTest'] != '') & (trailheadRef['AddressTest'] == ''), 'failCode'] = 'GEO31'\n",
    "\n",
    "##TODO: #25 fetch coordinates of trailheads using Nominatim address service\n",
    "##while trailheadRef['GeoTest'] is 'Fail':\n",
    "    ##do the thing\n",
    "\n",
    "##todo #9 write publisher for failed geoTest\n",
    "\n",
    "trailheadRef_clean = pd.DataFrame()\n",
    "trailheadRef_clean = trailheadRef[trailheadRef['failCode'] == '']\n",
    "trailheadRef_clean.drop(columns=['CoordinateTest','AddressTest','failCode'],inplace=True,errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trailhead eBird hotspot update\n",
    "\n",
    "1. Ask the eBird API for the latest list of hotspots for each trailhead\n",
    "2. Add new eBird hotspots to the table 'Hotspots' in the trailheadDirectBirds_sous database\n",
    "3. Update hotspot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get current hotspots from sous db\n",
    "sous_trailheadHotspots = pd.read_sql('select * from hotspots',cnx)\n",
    "sous_trailheadHotspots.set_index('index',inplace=True)\n",
    "sous_trailheadHotspots.convert_dtypes()\n",
    "##hard update on critical dtypes\n",
    "sous_trailheadHotspots['latestObsDt'] = sous_trailheadHotspots['latestObsDt'].astype('datetime64[ns]')\n",
    "sous_trailheadHotspots['latestUpdate'] = sous_trailheadHotspots['latestUpdate'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fetch all hotspots within 4 kilometers of each trailhead\n",
    "import time\n",
    "trailheadHotspots = []\n",
    "\n",
    "for StopName in trailheadRef_clean.itertuples():\n",
    "    time.sleep(0.3)\n",
    "    url = '''https://api.ebird.org/v2/ref/hotspot/geo?lat={}&lng={}&dist=4&fmt=json'''.format(StopName.Latitude,StopName.Longitude)\n",
    "    ebirdapi_auth_header = {'X-eBirdApiToken': ebird_token}\n",
    "    ebird_request = requests.get(url,headers=ebirdapi_auth_header)\n",
    "    ebird_response = pd.DataFrame(ebird_request.json())\n",
    "    if ebird_request.status_code == requests.codes.ok:\n",
    "        ebird_response['StopName'] = StopName.StopName\n",
    "        trailheadHotspots.append(ebird_response)\n",
    "    ebird_request.raise_for_status()\n",
    "\n",
    "all_trailheadHotspots = pd.concat(trailheadHotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_trailheadHotspots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11300/4226529427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##TODO #24 write script to pass in new hotspots and update existing hotspots --DONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestObsDt'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestObsDt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datetime64[ns]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestUpdate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_trailheadHotspots' is not defined"
     ]
    }
   ],
   "source": [
    "##TODO #24 write script to pass in new hotspots and update existing hotspots --DONE\n",
    "today = dt.datetime.today()\n",
    "pd.DataFrame(all_trailheadHotspots)\n",
    "all_trailheadHotspots['latestObsDt'] = all_trailheadHotspots['latestObsDt'].astype('datetime64[ns]')\n",
    "all_trailheadHotspots['latestUpdate'] = today.date()\n",
    "\n",
    "##sort, remove old data\n",
    "frames = [all_trailheadHotspots,sous_trailheadHotspots]\n",
    "hotspots = pd.concat(frames,ignore_index=True)\n",
    "hotspots.sort_values(by=['StopName','locId','latestObsDt','latestUpdate'],ascending=False,inplace=True)\n",
    "hotspots.drop_duplicates(subset=['StopName','locId'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load to cooking\n",
    "##this load into cooking triggers more scripts that update the production table 'Hotspots'\n",
    "hotspots.to_sql(name='Hotspots_cooking',con=cnx,if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a polygon for each hotspot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = 0.005    ##equivalent to 0.5 miles when applied\n",
    "hotspotsGeo = pd.DataFrame()\n",
    "hotspotsGeo = pd.read_sql('SELECT locId,lat,lng FROM Hotspots', con=cnx)\n",
    "##TODO #34 remove duplicate hotspots before building table of polygons for each hotspot --DONE\n",
    "##hotspotsGeo.set_index('index',inplace=True)\n",
    "hotspotsGeo.sort_values(by=['locId'],ascending=True,inplace=True)\n",
    "hotspotsGeo.drop_duplicates(subset=['locId'],keep='first',inplace=True)\n",
    "hotspotsGeo.reset_index()\n",
    "##TODO #33 write a function to incrament the lat & lng values as needed then output the results to a single column in the format lat,lng --DONE\n",
    "\n",
    "##Define functions to produce a corner of a polygon for each hotspot (NE,SE,SW,NW)\n",
    "def NW(x,y):\n",
    "    return x+diff,y-diff\n",
    "def NE(x,y):\n",
    "    return x+diff,y+diff\n",
    "def SE(x,y):\n",
    "    return x-diff,y+diff\n",
    "def SW(x,y):\n",
    "    return x-diff,y-diff\n",
    "\n",
    "##apply the functions as new columns\n",
    "##NOTE that appEEARS only accepts coordinates as (longitude,latitude) which is contrary to geoJSON documentation\n",
    "hotspotsGeo['NW'] = hotspotsGeo.apply(lambda i: NW(i.lng,i.lat), axis = 1).astype(str)\n",
    "hotspotsGeo['NE'] = hotspotsGeo.apply(lambda i: NE(i.lng,i.lat), axis = 1).astype(str)\n",
    "hotspotsGeo['SE'] = hotspotsGeo.apply(lambda i: SE(i.lng,i.lat), axis = 1).astype(str)\n",
    "hotspotsGeo['SW'] = hotspotsGeo.apply(lambda i: SW(i.lng,i.lat), axis = 1).astype(str)\n",
    "##TODO #35 (script to support) write the results to the database as a new table with a 'latestUpdate' column --DONE\n",
    "today = dt.datetime.today()\n",
    "hotspotsGeo['latestUpdate'] = today.date()\n",
    "hotspotsGeo['latestUpdate'] = hotspotsGeo['latestUpdate'].astype('datetime64[ns]')\n",
    "hotspotsGeo.to_sql(name='hotspotsGeo',con=cnx,if_exists='replace')\n",
    "\n",
    "##TODO #37 compile the polygon generated for each hotspot into a coordinate pack formatted for JSON insert --DONE\n",
    "for locId in hotspotsGeo.itertuples():\n",
    "    hotspotsGeo['polygon'] = '[[{},{},{},{},{}]]'.format(locId.NW,locId.NE,locId.SE,locId.SW,locId.NW).replace('(','[').replace(')',']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to the appEEARS service to fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #40 remove the getpass prompt and replace with a credential manager call\n",
    "earthdata_baseUrl = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'\n",
    "appEEARS_username = 'lwylie'\n",
    "appEEARS_password = 'BdiUPBhUa7ma5ds'\n",
    "import getpass\n",
    "NASA_username = getpass.getpass(prompt = 'Enter NASA Earthdata Login Username: ')\n",
    "NASA_password = getpass.getpass(prompt = 'Enter NASA Earthdata Login Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_type': 'Bearer', 'token': 'mB7FN6oHml3K2WPF-nqlyxrWpPL1Sfvct5C-E4v-1A2eWPbawYoKsKsKTJV1yj_vX9kJbsgY5hBgqnIFeJwCRw', 'expiration': '2022-02-10T22:22:17Z'}\n"
     ]
    }
   ],
   "source": [
    "##obtain an Earthdata token\n",
    "##TODO #36 the earthdata API is often under maintenance, write a script to abort this process if a new token cannot be obtain\n",
    "earthdata_loginURL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/login'\n",
    "earthdata_loginRequest = requests.post(earthdata_loginURL,auth=(NASA_username,NASA_password))\n",
    "earthdata_loginResponse = earthdata_loginRequest.json()\n",
    "print(earthdata_loginResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Transcribe taken, builder header\n",
    "earthdata_token = earthdata_loginResponse['token']\n",
    "earthdata_head = {'Authorization': 'Bearer {}'.format(earthdata_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##package coordinates as a geoJSON\n",
    "glhf = {\"type\": \"FeatureCollection\", \"features\":\n",
    "        [{\n",
    "        \"type\":\"Feature\",\n",
    "            \"geometry\":\n",
    "                {\"type\": \"Polygon\",\n",
    "                \"coordinates\":\n",
    "                    [[[-122.35524020000001, 47.646880599999996],\n",
    "                    [-122.35524020000001, 47.6568806],\n",
    "                    [-122.3652402, 47.6568806],\n",
    "                    [-122.3652402, 47.646880599999996],\n",
    "                    [-122.35524020000001, 47.646880599999996]]]\n",
    "                },\n",
    "            \"properties\": {}}]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"features\": [{\"geometry\": {\"coordinates\": [[[47.6568806, -122.3652402], [47.6568806, -122.3552402], [47.6468805, -122.3552402], [47.6468805, -122.3652402]]], \"type\": \"Polygon\"}, \"properties\": {}, \"type\": \"Feature\"}], \"type\": \"FeatureCollection\"}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ##TODO #41 resolve the null geometry issue. Start with inspecting shape file from demo in geopandas. Likely issue is formatting --DONE HOLY FUCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##JSON task template\n",
    "earthdata_task = {\n",
    "    'task_type': 'area',\n",
    "    'task_name': 'L10445835',\n",
    "    'params': {\n",
    "         'dates': \n",
    "            [{\"endDate\": \"12-31\", \n",
    "            \"recurring\": True, \n",
    "            \"startDate\": \"01-01\", \n",
    "            \"yearRange\": [2017, 2019]}],\n",
    "         'layers': \n",
    "            [{\"layer\": \"LC_Prop1\", \"product\": \"MCD12Q1.006\"}, \n",
    "            {\"layer\": \"LC_Type3\", \"product\": \"MCD12Q1.006\"}],\n",
    "         'output': {\n",
    "             'format': {\n",
    "                    'type': 'netcdf4'}, \n",
    "                    'projection': 'geographic'},\n",
    "         'geo':glhf,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'd3cd0222-c7d6-4a6d-ae2d-b2cade614934', 'status': 'pending'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##submit the task request\n",
    "earthdata_taskReq = requests.post('{}task'.format(earthdata_baseUrl),json=earthdata_task,headers=earthdata_head).json()\n",
    "earthdata_taskReq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Data Products:\n",
    "- LAI: LAI is a measure for the total area of leaves per unit ground area and directly related to the amount of light that can be intercepted by plants. It is defined as the one-sided green leaf area per unit ground surface area (LAI = leaf area / ground area, m2 / m2) in broadleaf canopies. There are three methods used to measure LAI for conifers; this project uses projected (or one-sided, in accordance the definition for broadleaf canopies) needle area per unit ground area.\n",
    "    - In general, a higher LAI value indicates more leaf coverage\n",
    "- fPAR: Fraction of absorbed photosynthetically active radiation (fPAR) is the fraction of incoming solar radiation in the spectrum of 400â€“700 nm that is absorbed by vegetation canopy. Data is provided as a percentage.\n",
    "- Land Cover Type 3: Annual Leaf Area Index (LAI) classification\n",
    "- FAO-Land Cover Classification System 1 (LCCS1) land cover layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #39 check status of the appEEARS task request\n",
    "params = {'limit': 2, 'pretty': True}\n",
    "earthdata_tasksResp = requests.get('{}task'.format(earthdata_baseUrl),params=params,headers=earthdata_head).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #38 get the results of the appEEARS task as a bundle\n",
    "taskID = '098dbb7a-0dfc-4f19-8410-a1db4f91170c'\n",
    "\n",
    "earthdata_bundle = requests.get('{}bundle/{}'.format(earthdata_baseUrl,taskID),).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f0d775b8-5a6c-40b7-919b-fb89525b076a': 'MCD12Q1-006-QC-lookup.csv',\n",
       " 'b17305d1-70d7-43f8-9ec9-b1a9f5172c9c': 'MCD12Q1-006-QC-Statistics-QA.csv',\n",
       " '5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62': 'MCD12Q1-006-LC-Prop1-Statistics.csv',\n",
       " 'fa1870c0-91ce-4467-9db6-c5e7c0502425': 'MCD12Q1-006-LC-Type3-Statistics.csv'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##filter for .csv's\n",
    "earthdata_files = {}\n",
    "for f in earthdata_bundle['files']:\n",
    "    if f['file_type'] in 'csv':\n",
    "        earthdata_files[f['file_id']] = f['file_name']\n",
    "    else: continue\n",
    "\n",
    "earthdata_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IT WORKS\n",
    "##To read results to a dataframe, pass in earthdata_baseUrl + 'bundle/ + taskID/ + file_id/ + file_name\n",
    "##example: https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv\n",
    "qrf = pd.DataFrame()\n",
    "qrf = pd.read_csv('https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>aid</th>\n",
       "      <th>Date</th>\n",
       "      <th>(5) Evergreen Broadleaf Forests</th>\n",
       "      <th>(7) Evergreen Needleleaf Forests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LC_Type3_2017001_aid0001</td>\n",
       "      <td>aid0001</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LC_Type3_2018001_aid0001</td>\n",
       "      <td>aid0001</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC_Type3_2019001_aid0001</td>\n",
       "      <td>aid0001</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File      aid        Date  \\\n",
       "0  LC_Type3_2017001_aid0001  aid0001  2017-01-01   \n",
       "1  LC_Type3_2018001_aid0001  aid0001  2018-01-01   \n",
       "2  LC_Type3_2019001_aid0001  aid0001  2019-01-01   \n",
       "\n",
       "   (5) Evergreen Broadleaf Forests  (7) Evergreen Needleleaf Forests  \n",
       "0                                2                                14  \n",
       "1                                2                                14  \n",
       "2                                2                                14  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##test files for L3943215\n",
    "exr = pd.DataFrame()\n",
    "exr = pd.read_csv('MCD12Q1-006-LC-Type3-Statistics.csv',sep=',')\n",
    "exr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##close the connection\n",
    "cnx.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce2b8b10e8082f390f0f7c9c12f304c9df3ed4554edd4b21c0fcee2d9ef65582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
