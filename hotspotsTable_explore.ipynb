{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import json\n",
    "import geojson\n",
    "import requests\n",
    "import base64\n",
    "import sqlite3 as db\n",
    "import cgi\n",
    "sys.path.append('../')\n",
    "\n",
    "pd.options.display.max_rows=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_userName = 'Tanag3r'\n",
    "ebird_token = 'j6c7l80ga2ib'\n",
    "cnx = db.connect('trailheadDirectBirds_sous.db')\n",
    "cur = cnx.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain & clean trailhead data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Route</th>\n",
       "      <th>StopName</th>\n",
       "      <th>Address</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>EastSunsetWay</td>\n",
       "      <td>661-831 E Sunset Way, Issaquah, WA 98027</td>\n",
       "      <td>47.529635</td>\n",
       "      <td>-122.025119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>HighSchool</td>\n",
       "      <td>Parking lot, The Rainier Trail, Issaquah, WA 9...</td>\n",
       "      <td>47.519345</td>\n",
       "      <td>-122.029801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>ChiricoTrail_PooPooPoint</td>\n",
       "      <td>11400 Issaquah-Hobart Road Southeast, Issaquah...</td>\n",
       "      <td>47.499949</td>\n",
       "      <td>-122.02173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>SquakMountain</td>\n",
       "      <td>13201 Squak Mountain Rd SE, Issaquah, WA 98027</td>\n",
       "      <td>47.481465</td>\n",
       "      <td>-122.053997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>NullMountain</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IssaquahAlps</td>\n",
       "      <td>MargaretsWay</td>\n",
       "      <td>190th Ave SE, Issaquah, WA 98027</td>\n",
       "      <td>47.50662</td>\n",
       "      <td>-122.08666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MountSi</td>\n",
       "      <td>MountSi</td>\n",
       "      <td></td>\n",
       "      <td>47.488966</td>\n",
       "      <td>-122.723044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MountSi</td>\n",
       "      <td>MountTeneriffe</td>\n",
       "      <td>Mount Teneriffe Rd, North Bend, WA 98045</td>\n",
       "      <td>47.490104</td>\n",
       "      <td>-122.709182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MountSi</td>\n",
       "      <td>LittleSi</td>\n",
       "      <td>SE Mt Si Rd, North Bend, WA 98045</td>\n",
       "      <td>47.489366</td>\n",
       "      <td>-122.753833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SeattleParks</td>\n",
       "      <td>DiscoveryParkNorth</td>\n",
       "      <td>Discovery Park Road &amp; Texas Way, Seattle, WA 9...</td>\n",
       "      <td>47.664631</td>\n",
       "      <td>-122.411063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SeattleParks</td>\n",
       "      <td>DiscoveryParkBeach</td>\n",
       "      <td>Fort Lawton Beach, Seattle, WA 98199</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SeattleParks</td>\n",
       "      <td>DiscoveryParkSouth</td>\n",
       "      <td>W Emerson St &amp; Magnolia Blvd W, Seattle, WA 98199</td>\n",
       "      <td>47.654152</td>\n",
       "      <td>-122.412975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Route                  StopName  \\\n",
       "index                                           \n",
       "0      IssaquahAlps             EastSunsetWay   \n",
       "1      IssaquahAlps                HighSchool   \n",
       "2      IssaquahAlps  ChiricoTrail_PooPooPoint   \n",
       "3      IssaquahAlps             SquakMountain   \n",
       "4      IssaquahAlps              NullMountain   \n",
       "5      IssaquahAlps              MargaretsWay   \n",
       "6           MountSi                   MountSi   \n",
       "7           MountSi            MountTeneriffe   \n",
       "8           MountSi                  LittleSi   \n",
       "9      SeattleParks        DiscoveryParkNorth   \n",
       "10     SeattleParks        DiscoveryParkBeach   \n",
       "11     SeattleParks        DiscoveryParkSouth   \n",
       "\n",
       "                                                 Address   Latitude  \\\n",
       "index                                                                 \n",
       "0               661-831 E Sunset Way, Issaquah, WA 98027  47.529635   \n",
       "1      Parking lot, The Rainier Trail, Issaquah, WA 9...  47.519345   \n",
       "2      11400 Issaquah-Hobart Road Southeast, Issaquah...  47.499949   \n",
       "3         13201 Squak Mountain Rd SE, Issaquah, WA 98027  47.481465   \n",
       "4                                                   <NA>       <NA>   \n",
       "5                       190th Ave SE, Issaquah, WA 98027   47.50662   \n",
       "6                                                         47.488966   \n",
       "7               Mount Teneriffe Rd, North Bend, WA 98045  47.490104   \n",
       "8                      SE Mt Si Rd, North Bend, WA 98045  47.489366   \n",
       "9      Discovery Park Road & Texas Way, Seattle, WA 9...  47.664631   \n",
       "10                  Fort Lawton Beach, Seattle, WA 98199       <NA>   \n",
       "11     W Emerson St & Magnolia Blvd W, Seattle, WA 98199  47.654152   \n",
       "\n",
       "        Longitude  \n",
       "index              \n",
       "0     -122.025119  \n",
       "1     -122.029801  \n",
       "2      -122.02173  \n",
       "3     -122.053997  \n",
       "4            <NA>  \n",
       "5      -122.08666  \n",
       "6     -122.723044  \n",
       "7     -122.709182  \n",
       "8     -122.753833  \n",
       "9     -122.411063  \n",
       "10           <NA>  \n",
       "11    -122.412975  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##pull in trailhead stops from trailheadDirectBirds_sous db\n",
    "trailheadRef = pd.DataFrame()\n",
    "trailheadRef = pd.read_sql('select * from trailheadRef;',cnx)\n",
    "trailheadRef.set_index('index',inplace=True)\n",
    "trailheadRef.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukew\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "##For the sake of demonstration please assume the data in the 'trailheadRef' comes from a King County Metro service\n",
    "##and/or is user-provided.\n",
    "\n",
    "##set types for error checking, other cleaning\n",
    "trailheadRef['Latitude'] = trailheadRef['Latitude'].fillna(0.00)\n",
    "trailheadRef['Longitude'] = trailheadRef['Longitude'].fillna(0.00)\n",
    "trailheadRef['Latitude'] = trailheadRef['Latitude'].astype('float64',errors='raise')\n",
    "trailheadRef['Longitude'] = trailheadRef['Longitude'].astype('float64',errors='raise')\n",
    "trailheadRef['CoordinateTest'] = ''\n",
    "trailheadRef['CoordinateTest'] = trailheadRef['CoordinateTest'].astype('str')\n",
    "trailheadRef['AddressTest'] = ''\n",
    "trailheadRef['AddressTest'] = trailheadRef['AddressTest'].astype('str')\n",
    "trailheadRef['failCode'] = ''\n",
    "trailheadRef['failCode'] = trailheadRef['failCode'].astype('str')\n",
    "\n",
    "##todo #10 finish this evaluator engine\n",
    "\n",
    "##warnings:\n",
    "trailheadRef.loc[(abs(trailheadRef['Latitude']) < 1) | (abs(trailheadRef['Longitude']) < 1), 'CoordinateTest'] = 'GEO10'\n",
    "trailheadRef.loc[(trailheadRef['Address'].isna()) | (trailheadRef['Address'].isnull()) | (trailheadRef['Address'] == ''), 'AddressTest'] = 'GEO20'\n",
    "##failures:\n",
    "trailheadRef.loc[(trailheadRef['CoordinateTest'] != '') & (trailheadRef['AddressTest'] != ''), 'failCode'] = 'GEO30'\n",
    "##update the test below when the coordinate fetch engine is complete to reflect failure of fetch\n",
    "trailheadRef.loc[(trailheadRef['CoordinateTest'] != '') & (trailheadRef['AddressTest'] == ''), 'failCode'] = 'GEO31'\n",
    "\n",
    "##TODO: #25 fetch coordinates of trailheads using Nominatim address service\n",
    "##while trailheadRef['GeoTest'] is 'Fail':\n",
    "    ##do the thing\n",
    "\n",
    "##todo #9 write publisher for failed geoTest\n",
    "\n",
    "trailheadRef_clean = pd.DataFrame()\n",
    "trailheadRef_clean = trailheadRef[trailheadRef['failCode'] == '']\n",
    "trailheadRef_clean.drop(columns=['CoordinateTest','AddressTest','failCode'],inplace=True,errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trailhead eBird hotspot update\n",
    "\n",
    "1. Ask the eBird API for the latest list of hotspots for each trailhead\n",
    "2. Add new eBird hotspots to the table 'Hotspots' in the trailheadDirectBirds_sous database\n",
    "3. Update hotspot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get current hotspots from sous db\n",
    "sous_trailheadHotspots = pd.read_sql('select * from hotspots',cnx)\n",
    "sous_trailheadHotspots.set_index('index',inplace=True)\n",
    "sous_trailheadHotspots.convert_dtypes()\n",
    "##hard update on critical dtypes\n",
    "sous_trailheadHotspots['latestObsDt'] = sous_trailheadHotspots['latestObsDt'].astype('datetime64[ns]')\n",
    "sous_trailheadHotspots['latestUpdate'] = sous_trailheadHotspots['latestUpdate'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fetch all hotspots within 4 kilometers of each trailhead\n",
    "import time\n",
    "trailheadHotspots = []\n",
    "\n",
    "for StopName in trailheadRef_clean.itertuples():\n",
    "    time.sleep(0.3)\n",
    "    url = '''https://api.ebird.org/v2/ref/hotspot/geo?lat={}&lng={}&dist=4&fmt=json'''.format(StopName.Latitude,StopName.Longitude)\n",
    "    ebirdapi_auth_header = {'X-eBirdApiToken': ebird_token}\n",
    "    ebird_request = requests.get(url,headers=ebirdapi_auth_header)\n",
    "    ebird_response = pd.DataFrame(ebird_request.json())\n",
    "    if ebird_request.status_code == requests.codes.ok:\n",
    "        ebird_response['StopName'] = StopName.StopName\n",
    "        trailheadHotspots.append(ebird_response)\n",
    "    ebird_request.raise_for_status()\n",
    "\n",
    "all_trailheadHotspots = pd.concat(trailheadHotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_trailheadHotspots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11300/4226529427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##TODO #24 write script to pass in new hotspots and update existing hotspots --DONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtoday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestObsDt'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestObsDt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datetime64[ns]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mall_trailheadHotspots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestUpdate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_trailheadHotspots' is not defined"
     ]
    }
   ],
   "source": [
    "##TODO #24 write script to pass in new hotspots and update existing hotspots --DONE\n",
    "today = dt.datetime.today()\n",
    "pd.DataFrame(all_trailheadHotspots)\n",
    "all_trailheadHotspots['latestObsDt'] = all_trailheadHotspots['latestObsDt'].astype('datetime64[ns]')\n",
    "all_trailheadHotspots['latestUpdate'] = today.date()\n",
    "\n",
    "##sort, remove old data\n",
    "frames = [all_trailheadHotspots,sous_trailheadHotspots]\n",
    "hotspots = pd.concat(frames,ignore_index=True)\n",
    "hotspots.sort_values(by=['StopName','locId','latestObsDt','latestUpdate'],ascending=False,inplace=True)\n",
    "hotspots.drop_duplicates(subset=['StopName','locId'],keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load to cooking\n",
    "##this load into cooking triggers more scripts that update the production table 'Hotspots'\n",
    "hotspots.to_sql(name='Hotspots_cooking',con=cnx,if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a polygon for each hotspot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "Error binding parameter 4 - probably unsupported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14016/1681850037.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mhotspotsGeo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestUpdate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mhotspotsGeo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestUpdate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhotspotsGeo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latestUpdate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'datetime64[ns]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mhotspotsGeo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hotspotsGeo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcnx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m##TODO #37 compile the polygon generated for each hotspot into a coordinate pack formatted for JSON insert --DONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2870\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2872\u001b[1;33m         sql.to_sql(\n\u001b[0m\u001b[0;32m   2873\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2874\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m         )\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m     pandas_sql.to_sql(\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, **kwargs)\u001b[0m\n\u001b[0;32m   2224\u001b[0m         )\n\u001b[0;32m   2225\u001b[0m         \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2226\u001b[1;33m         \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhas_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, chunksize, method)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m                 \u001b[0mexec_insert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     def _query_iterator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[1;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[0;32m   1922\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_insert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1924\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_statement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_insert_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInterfaceError\u001b[0m: Error binding parameter 4 - probably unsupported type."
     ]
    }
   ],
   "source": [
    "diff = 0.005    ##equivalent to 0.5 miles when applied\n",
    "hotspotsGeo = pd.DataFrame()\n",
    "hotspotsGeo = pd.read_sql('SELECT locId,lat,lng FROM Hotspots', con=cnx)\n",
    "##TODO #34 remove duplicate hotspots before building table of polygons for each hotspot --DONE\n",
    "##hotspotsGeo.set_index('index',inplace=True)\n",
    "hotspotsGeo.sort_values(by=['locId'],ascending=True,inplace=True)\n",
    "hotspotsGeo.drop_duplicates(subset=['locId'],keep='first',inplace=True)\n",
    "hotspotsGeo.reset_index()\n",
    "##TODO #33 write a function to incrament the lat & lng values as needed then output the results to a single column in the format lat,lng --DONE\n",
    "\n",
    "##Define functions to produce a corner of a polygon for each hotspot (NE,SE,SW,NW)\n",
    "def NW(x,y):\n",
    "    return x-diff,y+diff\n",
    "def NE(x,y):\n",
    "    return x+diff,y+diff\n",
    "def SE(x,y):\n",
    "    return x+diff,y-diff\n",
    "def SW(x,y):\n",
    "    return x-diff,y-diff\n",
    "\n",
    "##apply the functions as new columns\n",
    "##NOTE that appEEARS only accepts coordinates as (longitude,latitude) which is contrary to geoJSON documentation\n",
    "hotspotsGeo['NW'] = hotspotsGeo.apply(lambda i: NW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['NE'] = hotspotsGeo.apply(lambda i: NE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['SE'] = hotspotsGeo.apply(lambda i: SE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['SW'] = hotspotsGeo.apply(lambda i: SW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "##TODO #35 (script to support) write the results to the database as a new table with a 'latestUpdate' column --DONE\n",
    "today = dt.datetime.today()\n",
    "hotspotsGeo['latestUpdate'] = today.date()\n",
    "hotspotsGeo['latestUpdate'] = hotspotsGeo['latestUpdate'].astype('datetime64[ns]')\n",
    "##hotspotsGeo.to_sql(name='hotspotsGeo',con=cnx,if_exists='append')\n",
    "\n",
    "##TODO #37 compile the polygon generated for each hotspot into a coordinate pack formatted for JSON insert --DONE\n",
    "##no longer neccessary, geoJSON handles packaging\n",
    "##for locId in hotspotsGeo.itertuples():\n",
    "    ##hotspotsGeo['polygon'] = '[[{},{},{},{},{}]]'.format(locId.NW,locId.NE,locId.SE,locId.SW,locId.NW).replace('(','[').replace(')',']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to the appEEARS service to fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #40 remove the getpass prompt and replace with a credential manager call\n",
    "earthdata_baseUrl = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'\n",
    "appEEARS_username = 'lwylie'\n",
    "appEEARS_password = 'BdiUPBhUa7ma5ds'\n",
    "import getpass\n",
    "NASA_username = getpass.getpass(prompt = 'Enter NASA Earthdata Login Username: ')\n",
    "NASA_password = getpass.getpass(prompt = 'Enter NASA Earthdata Login Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_type': 'Bearer', 'token': 'fnfJNXOWdPcUloYPa2rIaz4F8IEQsOouanowKmhGMpjvOkB8XVeWxS9SgDkKWF3WdFBijE7e8wrZBkHykX4MXQ', 'expiration': '2022-02-12T22:31:44Z'}\n"
     ]
    }
   ],
   "source": [
    "##obtain an Earthdata token\n",
    "##TODO #36 the earthdata API is often under maintenance, write a script to abort this process if a new token cannot be obtain\n",
    "earthdata_loginURL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/login'\n",
    "earthdata_loginRequest = requests.post(earthdata_loginURL,auth=(NASA_username,NASA_password))\n",
    "earthdata_loginResponse = earthdata_loginRequest.json()\n",
    "print(earthdata_loginResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Transcribe token, builder header\n",
    "earthdata_token = earthdata_loginResponse['token']\n",
    "earthdata_head = {'Authorization': 'Bearer {}'.format(earthdata_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #43 write a cooked table of locId's with stats to check against list of hotspots. Drive fetch off difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "1. Make all requests in a loop, producing a dictionary of {'locId': 'earthdata_taskID'}\n",
    "2. For each locId, pull each .csv into a dataframe then load that dataframe into a database table bearing the name that corresponds with the layer and product. Append the locId.\n",
    "    EXAMPLE: the contents of the .csv file for 'MCD12Q1-006-LC-Prop1-Statistics.csv' go into the table 'MCD12Q1-006-LC-Prop1-Statistics' in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "##small frame of hotspots for testing\n",
    "hotspotsGeo_short = hotspotsGeo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14016/443713273.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log --DONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mearthdata_taskReq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}task'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mearthdata_baseUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearthdata_task\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearthdata_head\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mED_taskResp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mearthdata_taskReq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m##TODO #46 something about this is broken, fix it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m             \u001b[1;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     return arrays_to_mgr(\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"If using all scalar values, you must pass an index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "##TODO #44 resolve error: {'message': \"RecursionError('maximum recursion depth exceeded in __instancecheck__',)\"} --DONE\n",
    "##TODO #45 once all hotspots/locId's have Prop1 and Type3 MCD12Q1 data, convert this block to check for LAI and fPAR\n",
    "earthdataTaskList = []\n",
    "earthdata_errorLog = []\n",
    "for locId in hotspotsGeo_short.itertuples():\n",
    "    ##time.sleep(5.0)\n",
    "    ##package polygon as a geoJSON\n",
    "    yy = locId.locId\n",
    "    glhf = {\"type\": \"FeatureCollection\", \"features\":\n",
    "        [{\n",
    "        \"type\":\"Feature\",\n",
    "            \"geometry\":\n",
    "                {\"type\": \"Polygon\",\n",
    "                \"coordinates\":\n",
    "                    '[[locId.NW,locId.NE,locId.SE,locId.SW,locId.NW]]'\n",
    "                },\n",
    "            \"properties\": {}}]\n",
    "        }\n",
    "    glhf_json = geojson.GeoJSON(glhf)\n",
    "    ##compile JSON task request\n",
    "    earthdata_task = {\n",
    "        'task_type': 'area',\n",
    "        'task_name': yy,\n",
    "        'params': {\n",
    "            'dates': \n",
    "                [{\"endDate\": \"12-31\", \n",
    "                \"recurring\": True, \n",
    "                \"startDate\": \"01-01\", \n",
    "                \"yearRange\": [2017, 2019]}],\n",
    "            'layers': \n",
    "                [{\"layer\": \"LC_Prop1\", \"product\": \"MCD12Q1.006\"}, \n",
    "                {\"layer\": \"LC_Type3\", \"product\": \"MCD12Q1.006\"}],\n",
    "            'output': {\n",
    "                'format': {\n",
    "                    'type': 'netcdf4'}, \n",
    "                    'projection': 'geographic'},\n",
    "        'geo':glhf_json}}\n",
    "    ##submit the task request\n",
    "    \n",
    "##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log --DONE\n",
    "earthdata_taskReq = requests.post('{}task'.format(earthdata_baseUrl),json=earthdata_task,headers=earthdata_head)\n",
    "ED_taskResp = pd.DataFrame(earthdata_taskReq.json())\n",
    "\n",
    "##TODO #46 something about this is broken, fix it\n",
    "if earthdata_taskReq.status_code == 202:\n",
    "    ##ED_taskResp = pd.DataFrame(earthdata_taskReq.json())\n",
    "    earthdataTaskList.append(ED_taskResp)\n",
    "else: \n",
    "    ##ED_taskResp = pd.DataFrame(earthdata_taskReq.json())\n",
    "    ED_taskResp['statusCode'] = earthdata_taskReq.status_code\n",
    "    ED_taskResp['request'] = json.dumps(earthdata_task)\n",
    "    ED_taskResp['reqDate'] = dt.datetime.today()\n",
    "    earthdata_errorLog.append(ED_taskResp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##compile either the error list or continue with the tasks\n",
    "earthdataTaskList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                             message  statusCode  \\\n",
       " 1  RecursionError('maximum recursion depth exceed...         500   \n",
       " \n",
       "                                              request  \\\n",
       " 1  {\"task_type\": \"area\", \"task_name\": \"L10129002\"...   \n",
       " \n",
       "                      reqDate  \n",
       " 1 2022-02-11 16:06:48.048480  ]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthdata_errorLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "##compile either the error list or continue with the tasks\n",
    "\n",
    "EDTaskList = pd.DataFrame()\n",
    "try:\n",
    "    EDTaskList = pd.concat(earthdataTaskList)\n",
    "except ValueError: \n",
    "    try:\n",
    "        earthdata_errorLog = pd.concat(earthdata_errorLog)\n",
    "        earthdata_errorLog.to_sql('earthdata_errorlog',con=cnx,if_exists='append')\n",
    "    except ValueError: raise Exception\n",
    "else: pd.concat(earthdataTaskList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14016/1589149991.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'limit'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pretty'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mearthdata_tasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}task'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mearthdata_baseUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearthdata_head\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'task_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'created'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'task_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "##submit the task request\n",
    "##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log\n",
    "params = {'limit': 15, 'pretty': True}\n",
    "earthdata_tasks = requests.get('{}task'.format(earthdata_baseUrl),params=params,headers=earthdata_head).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Data Products:\n",
    "- LAI: LAI is a measure for the total area of leaves per unit ground area and directly related to the amount of light that can be intercepted by plants. It is defined as the one-sided green leaf area per unit ground surface area (LAI = leaf area / ground area, m2 / m2) in broadleaf canopies. There are three methods used to measure LAI for conifers; this project uses projected (or one-sided, in accordance the definition for broadleaf canopies) needle area per unit ground area.\n",
    "    - In general, a higher LAI value indicates more leaf coverage\n",
    "- fPAR: Fraction of absorbed photosynthetically active radiation (fPAR) is the fraction of incoming solar radiation in the spectrum of 400–700 nm that is absorbed by vegetation canopy. Data is provided as a percentage.\n",
    "- Land Cover Type 3: Annual Leaf Area Index (LAI) classification\n",
    "- FAO-Land Cover Classification System 1 (LCCS1) land cover layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for task_id in earthdataTaskList.itertuples():\n",
    "##TODO #39 check status of the appEEARS task request --DONE\n",
    "import time\n",
    "earthdata_tasksResp = requests.get('{}task/{}'.format(earthdata_baseUrl,earthdata_taskID),headers=earthdata_head).json()\n",
    "starttime = time.time()\n",
    "while earthdata_tasksResp['status'] !='done':\n",
    "    print(requests.get('{}task/{}'.format(earthdata_baseUrl,earthdata_taskID),headers=earthdata_head).json()['status'])\n",
    "    time.sleep(20.0 - ((time.time() - starttime) % 20.0))\n",
    "print(requests.get('{}task/{}'.format(earthdata_baseUrl,earthdata_taskID),headers=earthdata_head).json()['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #38 get the results of the appEEARS task as a bundle\n",
    "earthdata_bundle = requests.get('{}bundle/{}'.format(earthdata_baseUrl,earthdata_taskID),).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'da025ae0-405e-4f31-b2ac-267b893c343d': 'MCD12Q1-006-QC-lookup.csv',\n",
       " '7bafef38-5c43-4c95-b786-a42062a42fd4': 'MCD12Q1-006-QC-Statistics-QA.csv',\n",
       " '3dad5532-8fb8-4cd8-ad45-7bb5777d5da8': 'MCD12Q1-006-LC-Prop1-Statistics.csv',\n",
       " '4668d16c-b9a3-4fa8-80bd-3ea1d569cf52': 'MCD12Q1-006-LC-Type3-Statistics.csv'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##filter for .csv's\n",
    "earthdata_files = {}\n",
    "for f in earthdata_bundle['files']:\n",
    "    if f['file_type'] in 'csv':\n",
    "        earthdata_files[f['file_id']] = f['file_name']\n",
    "    else: continue\n",
    "\n",
    "earthdata_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_id in earthdata_files:\n",
    "    allresults = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IT WORKS\n",
    "##To read results to a dataframe, pass in earthdata_baseUrl + 'bundle/ + taskID/ + file_id/ + file_name\n",
    "##example: https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv\n",
    "qrf = pd.DataFrame()\n",
    "qrf = pd.read_csv('https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##close the connection\n",
    "cnx.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce2b8b10e8082f390f0f7c9c12f304c9df3ed4554edd4b21c0fcee2d9ef65582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
