{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json\n",
    "import geojson\n",
    "import requests\n",
    "import sqlite3 as db\n",
    "\n",
    "pd.options.display.max_rows=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_userName = 'Tanag3r'\n",
    "ebird_token = 'j6c7l80ga2ib'\n",
    "db_name = 'trailheadDirectBirds_sous.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##connect to database\n",
    "def connectDB():\n",
    "    try:\n",
    "        cnx = db.connect(db_name)\n",
    "    except Exception as cnxError:\n",
    "        raise UserWarning(f'Unable to connect to database due to: {cnxError}')\n",
    "    return cnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Data Products:\n",
    "- LAI: LAI is a measure for the total area of leaves per unit ground area and directly related to the amount of light that can be intercepted by plants. It is defined as the one-sided green leaf area per unit ground surface area (LAI = leaf area / ground area, m2 / m2) in broadleaf canopies. There are three methods used to measure LAI for conifers; this project uses projected (or one-sided, in accordance the definition for broadleaf canopies) needle area per unit ground area.\n",
    "    - In general, a higher LAI value indicates more leaf coverage\n",
    "- fPAR: Fraction of absorbed photosynthetically active radiation (fPAR) is the fraction of incoming solar radiation in the spectrum of 400â€“700 nm that is absorbed by vegetation canopy. Data is provided as a percentage.\n",
    "- Land Cover Type 3: Annual Leaf Area Index (LAI) classification\n",
    "- FAO-Land Cover Classification System 1 (LCCS1) land cover layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #40 remove the getpass prompt and replace with a credential manager call\n",
    "#earthdata_baseUrl = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'  #depracated\n",
    "earthdata_baseUrl = 'https://appeears.earthdatacloud.nasa.gov/api/'\n",
    "appEEARS_username = 'lwylie'\n",
    "appEEARS_password = 'BdiUPBhUa7ma5ds'\n",
    "import getpass\n",
    "NASA_username = getpass.getpass(prompt = 'Enter NASA Earthdata Login Username: ')\n",
    "NASA_password = getpass.getpass(prompt = 'Enter NASA Earthdata Login Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_type': 'Bearer', 'token': 'sxp0K1dJbKqo3lx0oTLkA0T-v0rQyQ2ygwv1La4xKOlWJmPVm2_Qd74-zheBjESyGzrvsb2oNw1BIdxiTo3-mg', 'expiration': '2022-05-11T21:26:19Z'}\n"
     ]
    }
   ],
   "source": [
    "##obtain an Earthdata token\n",
    "##TODO #36 the earthdata API is often under maintenance, write a script to abort this process if a new token cannot be obtain\n",
    "##TODO #56 refactor block into a function that requests a token from the earthdata service and returns the header on success\n",
    "earthdata_loginURL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/login/'\n",
    "earthdata_cloudLoginURL = 'https://appeears.earthdatacloud.nasa.gov/api/login'\n",
    "earthdata_loginRequest = requests.post(earthdata_cloudLoginURL,auth=(NASA_username,NASA_password))\n",
    "earthdata_loginResponse = earthdata_loginRequest.json()\n",
    "print(earthdata_loginResponse)\n",
    "##Transcribe token, builder header\n",
    "earthdata_token = earthdata_loginResponse['token']\n",
    "earthdata_head = {'Authorization': 'Bearer {}'.format(earthdata_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the earthdata dictionary from the database and return a list of locId's without Prop1 and Type3 data\n",
    "def list_needMODIS(testList: list):\n",
    "    cnx = connectDB()\n",
    "    diff = 0.005    #equivalent to half a mile\n",
    "    try:\n",
    "        queryList = []\n",
    "        for i in testList:\n",
    "            i = str(i)\n",
    "            queryList.append(i)\n",
    "        queryList = str(queryList).strip('[]')\n",
    "        #dataframe of hotspots from the database\n",
    "        hotspotsGeo = pd.read_sql(f'SELECT locId,lat,lng FROM Hotspots WHERE locId not in (SELECT DISTINCT locId from lpdaac_dictionary WHERE LPDAACkey in ({queryList}));', con=cnx)\n",
    "        if hotspotsGeo.empty == True:\n",
    "            raise Exception(f'An empty dataframe has been returned')\n",
    "        else:\n",
    "            hotspotsGeo.sort_values(by=['locId'],ascending=True,inplace=True)\n",
    "            hotspotsGeo.drop_duplicates(subset=['locId'],keep='first',inplace=True)\n",
    "            hotspotsGeo.reset_index()\n",
    "        #build out squares around each hotspot\n",
    "            def NW(x,y):\n",
    "                return x-diff,y+diff\n",
    "            def NE(x,y):\n",
    "                return x+diff,y+diff\n",
    "            def SE(x,y):\n",
    "                return x+diff,y-diff\n",
    "            def SW(x,y):\n",
    "                return x-diff,y-diff\n",
    "        ##apply the functions as new columns\n",
    "        ##NOTE that appEEARS only accepts coordinates as (longitude,latitude) which is contrary to geoJSON documentation\n",
    "            hotspotsGeo['NW'] = hotspotsGeo.apply(lambda i: NW(i.lng,i.lat), axis = 1)\n",
    "            hotspotsGeo['NE'] = hotspotsGeo.apply(lambda i: NE(i.lng,i.lat), axis = 1)\n",
    "            hotspotsGeo['SE'] = hotspotsGeo.apply(lambda i: SE(i.lng,i.lat), axis = 1)\n",
    "            hotspotsGeo['SW'] = hotspotsGeo.apply(lambda i: SW(i.lng,i.lat), axis = 1)\n",
    "    \n",
    "    except Exception as exd:\n",
    "        raise UserWarning(f'An unexpected error occurred in the function list_needMODIS: {exd}')\n",
    "    return hotspotsGeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "1. Make all requests in a loop, producing a list of {'locId': 'earthdata_taskID'}\n",
    "2. For each locId, pull each .csv into a dataframe then load that dataframe into a database table bearing the name that corresponds with the layer and product. Append the locId.\n",
    "    EXAMPLE: the contents of the .csv file for 'MCD12Q1-006-LC-Prop1-Statistics.csv' go into the table 'MCD12Q1-006-LC-Prop1-Statistics' in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #59 update input on geoPack function to take a list of coordinates\n",
    "def geoPack(NW,NE,SE,SW):\n",
    "    try:\n",
    "        geoPack_wrap = {\"type\": \"FeatureCollection\", \"features\":\n",
    "            [{\n",
    "            \"type\":\"Feature\",\n",
    "                \"geometry\":\n",
    "                    {\"type\": \"Polygon\",\n",
    "                    \"coordinates\":\n",
    "                        [[NW,NE,SE,SW,NW]]\n",
    "                    },\n",
    "                \"properties\": {}}]\n",
    "            }\n",
    "    except geojson.GeoJSON(geoPack_wrap).is_valid == False:\n",
    "        print(geojson.GeoJSON(geoPack_wrap).errors)\n",
    "    return geojson.GeoJSON(geoPack_wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #60 update 'task' function to include layers as an input\n",
    "def task(taskName: str,endDate: str,startDate: str,recurring: bool,yearRange: list, geoPack_wrap = geojson.GeoJSON):\n",
    "    try:\n",
    "        edTask = {\n",
    "            'task_type': 'area',\n",
    "            'task_name': taskName,\n",
    "            'params': {\n",
    "                'dates': \n",
    "                    [{\"endDate\": endDate, \n",
    "                    \"recurring\": recurring, \n",
    "                    \"startDate\": startDate, \n",
    "                    \"yearRange\": yearRange}],\n",
    "                'layers': \n",
    "                    [{\"layer\": \"FparExtra_QC\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"FparLai_QC\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"FparStdDev_500m\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"Fpar_500m\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"LaiStdDev_500m\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"Lai_500m\", \"product\": \"MOD15A2H.006\"}],\n",
    "                'output': {\n",
    "                    'format': {\n",
    "                        'type': 'netcdf4'}, \n",
    "                        'projection': 'geographic'},\n",
    "            'geo':geoPack_wrap}}\n",
    "    except Exception as JSONerror:\n",
    "        raise UserWarning(JSONerror)\n",
    "    return edTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##error logger function\n",
    "def log_earthdataError(message,statusCode: int,request: str,reqDate):\n",
    "    cnx = connectDB()\n",
    "    cur = cnx.cursor()\n",
    "    try:\n",
    "        sqliteInsert = 'INSERT INTO earthdata_errorlog (message,statusCode,request,reqDate) VALUES (?, ?, ?, ?)'\n",
    "        logTuple = (message,statusCode,request,reqDate)\n",
    "        cur.execute(sqliteInsert,logTuple)\n",
    "        cnx.commit()\n",
    "        cur.close()\n",
    "    except db.Error as sqlError:\n",
    "        raise UserWarning(sqlError)\n",
    "    finally:\n",
    "        if cnx:\n",
    "            cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #45 once all hotspots/locId's have Prop1 and Type3 MCD12Q1 data, convert this block to check for LAI and fPAR\n",
    "\n",
    "#the testList variable requires a str list of tests from the LPDAAC keys table\n",
    "def post_earthdataTask(testList: list):\n",
    "    import time\n",
    "    #hotspotsGeo = hotspotsGeo_short    #small frame of hotspots for testing\n",
    "    try:\n",
    "        hotspotsGeo = pd.DataFrame(list_needMODIS(testList=testList)).head(2)\n",
    "        earthdataTaskList = []\n",
    "        for locId in hotspotsGeo.itertuples():\n",
    "            time.sleep(0.5)\n",
    "            coords = geoPack(NW = locId.NW,NE = locId.NE,SE = locId.SE,SW = locId.SW)\n",
    "            earthdataTask = task(taskName = locId.locId,endDate = \"12-31\",startDate=\"01-01\",recurring=True,yearRange=[2019,2021],geoPack_wrap=coords)\n",
    "            taskReq = requests.post(f'{earthdata_baseUrl}task',json=earthdataTask,headers=earthdata_head)\n",
    "            if taskReq.status_code != 200:\n",
    "                log_earthdataError(str(taskReq),int(taskReq.status_code),json.dumps(earthdataTask),dt.datetime.today())\n",
    "            earthdataTaskList.append(taskReq.json())\n",
    "    except Exception as ee:\n",
    "        raise ee\n",
    "    return earthdataTaskList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earthdataTask(testList: list,cnx=connectDB()):\n",
    "    #postedTasks = pd.DataFrame(help) #for testing\n",
    "    taskInfo = []\n",
    "    fileList = []\n",
    "    loads = []\n",
    "    postedTasks = pd.DataFrame(post_earthdataTask(testList=testList))\n",
    "    import time\n",
    "    try:\n",
    "        for task_id in postedTasks.itertuples():\n",
    "            while requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()['status'] !='done':\n",
    "                requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()['status']\n",
    "                time.sleep(20.0)\n",
    "            taskStatus = requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()\n",
    "            taskInfo.append(taskStatus)\n",
    "        taskBatch = pd.DataFrame(data=taskInfo,columns=['task_id','task_name','status','completed'])\n",
    "\n",
    "        for task_id in taskBatch.itertuples():\n",
    "            earthdata_bundle = requests.get(f'{earthdata_baseUrl}bundle/{task_id.task_id}',headers=earthdata_head).json()\n",
    "            ##filter down to .csv files\n",
    "            for x in earthdata_bundle['files']:\n",
    "                if x['file_type'] in 'csv':\n",
    "                    time.sleep(0.3)\n",
    "                    fileID = x['file_id']\n",
    "                    fileName = x['file_name']\n",
    "                    loads.append({task_id.task_name,task_id.task_id,fileID,fileName})\n",
    "                    getIt_url = requests.get(f'https://appeears.earthdatacloud.nasa.gov/api/bundle/{task_id.task_id}/{fileID}',headers=earthdata_head,allow_redirects=True,stream=True).url\n",
    "                    getIt = pd.read_csv(getIt_url)\n",
    "                    getIt['locId'] = task_id.task_name\n",
    "                    getIt['latestUpdate'] = str(dt.datetime.today())\n",
    "                    try:\n",
    "                        getIt.to_sql(name=f'{fileName}_MODIS_cooking',con=cnx,if_exists='append')\n",
    "                        cnx.commit()\n",
    "                    ##TODO #53 test the exception --DONE\n",
    "                    ##TODO #95 update exception in get_earthdataTask to get the AWS URL if one is produced and log the url in the db\n",
    "                    except db.DatabaseError as resultError:\n",
    "                        fileList.append({resultError: resultError.__cause__,'taskName':task_id.task_name,'url':getIt_url})\n",
    "                        pass\n",
    "                else: continue  \n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return fileList,loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{OperationalError('table MOD15A2H-006-FparExtra-QC-Statistics-QA.csv_MODIS_cooking has no column named 129'): None, 'taskName': 'L1855923', 'url': 'https://d2v96idf0qj40l.cloudfront.net/s3-eab5ea0d219da2588ae2241aa105a663/appeears-output.s3.amazonaws.com/70c9cf66-2be0-4ee1-a668-c309fd5328f8/MOD15A2H-006-FparExtra-QC-Statistics-QA.csv?response-content-disposition=attachment%253B%2520filename%253DMOD15A2H-006-FparExtra-QC-Statistics-QA.csv&AWSAccessKeyId=ASIA3VC75OCO4EZCUFBW&Signature=bLZlJsmXiIX99mtl6JoWI2xnUME%253D&x-amz-security-token=IQoJb3JpZ2luX2VjEEMaCXVzLXdlc3QtMiJHMEUCIQC1b%252BMwpt6O%252FRA3NAj01rVbIfzDmxdYS%252Fv3JYZAxiQnVAIgGR67eK0adQJjzhl5ehCqaNQEBRJ4SRKOOAZep%252F%252FIjLUqiQQIHBAAGgw4MDEyMTI1NTk1MTciDK7dKSRkPRWux93O5yrmA3qInKb9uU%252BLCcDTfKUP6rVg1qNhlMRenRih0zrPwhDH9qDbzhnvI8mowT8ENMsjx6AndrL3YIcg7W42AbUF6HgbnINm4f4Z3BiA91p4T455%252FAKbzWG8w%252BW1VmSKeogBHFSGJoA3iHE4Owp3ON8Lu1Ln3AgWbTUQLqFvgRUXOySSDvB8MccQ10wWd0frOGfjLho5UvYsXHA8bZloAhnvfHUUeporBZLxZo%252F9ZX%252FazbFY3zk7fnHrdFNk87rNV0nYqwcTYxrfgX1pmbpZjxZxRRh28A7Nr3hQxkBBmnW8ihZv%252Bmb9LqcUGtQiLCeMnMcPucsFPWFe9q27sA8m2JgB85mCuxgZUkzXVkLBlkOHHu8Vx4bnaL6PGOZZlhr8JB2Hgvg%252BHxx8IaVt%252BKwSpcMB29%252FhSmZKROWZtagZ0aqHSLHQq7mvGYOyT8OM7qFruvPR9OTEzMnO3EfEiCUC40lY88GopR13ld7VE2dFNlYTDoQ0L1C0seszluGI7PPJvWZwBl5LmhN%252FtVZ6Bg0c76Gxg1KYoqj%252Fy4SUsVYduq%252FNqu%252BHdDUSx7lRUVFJu36yfCJiiHDSt8wNV8tbuSq8%252BVeUIt8Xswh3mLzX1TJt3VRE%252BOio6qPrAQmg0k9r1WzFjlqKT7KQlVh7TDDV3eqTBjqlAdZYhu%252FuAqsaeQGkgPRdHOzCri%252F0X51HS4k6DixDhk73q15V0nGGqz08sIhdVrDLLQiMhwXN7%252BHwv0KlF%252Bn%252FHkAv9csxS27ID%252BXcCnOxB54mRr5yh60%252BbqA8MwlWvdq7iNCNXZr9EiltpXG%252B9pfkenul3f4aRuH0rMP2l%252FZ8nolIQW0ZPte3hm80Qt1ZzyDf6yZ4NZQNrzkxTBA9OmWKFgtQfyDbCg%253D%253D&Expires=1652387234'}]\n",
      "[{'8bef1eac-7b9a-463d-a118-a9d66d03960a', 'L1855923', '70c9cf66-2be0-4ee1-a668-c309fd5328f8', 'MOD15A2H-006-FparExtra-QC-lookup.csv'}, {'MOD15A2H-006-FparLai-QC-lookup.csv', 'L1855923', '70c9cf66-2be0-4ee1-a668-c309fd5328f8', '0a5c306b-2b60-437e-8f6e-fd50b359e19d'}, {'MOD15A2H-006-FparExtra-QC-Statistics-QA.csv', 'L1855923', '70c9cf66-2be0-4ee1-a668-c309fd5328f8', '8c0701ea-aaea-487f-9528-b858db8f77f1'}, {'e49a0be7-3daf-4768-8e81-19e8e9604b3c', 'L1855923', '70c9cf66-2be0-4ee1-a668-c309fd5328f8', 'MOD15A2H-006-FparLai-QC-Statistics-QA.csv'}, {'MOD15A2H-006-Statistics.csv', 'L1855923', '70c9cf66-2be0-4ee1-a668-c309fd5328f8', '5be19116-b67f-494b-8b16-03ecea577a50'}, {'6c0d31b0-2459-4968-af41-32efa6f11556', 'ec3d0b9a-3e53-4028-baa1-83e1ee1c2178', 'L1875789', 'MOD15A2H-006-FparExtra-QC-lookup.csv'}, {'6c0d31b0-2459-4968-af41-32efa6f11556', 'MOD15A2H-006-FparLai-QC-lookup.csv', 'L1875789', '6ae234f3-dba8-48b4-a564-400509f05f08'}, {'6c0d31b0-2459-4968-af41-32efa6f11556', 'MOD15A2H-006-FparExtra-QC-Statistics-QA.csv', 'L1875789', 'bc4d450b-9a73-46e3-9eac-8ec4b465336f'}, {'6c0d31b0-2459-4968-af41-32efa6f11556', 'L1875789', '0aba51e4-cf0e-4407-90b2-5a6dd72d508a', 'MOD15A2H-006-FparLai-QC-Statistics-QA.csv'}, {'6c0d31b0-2459-4968-af41-32efa6f11556', 'L1875789', 'MOD15A2H-006-Statistics.csv', '1c4279f3-1f00-46d5-ab83-9493f766b68a'}]\n"
     ]
    }
   ],
   "source": [
    "fileList,loads = get_earthdataTask(testList=[\"FparExtra_QC\",\"FparLai_QC\",\"FparStdDev_500m\",\"Fpar_500m\",\"LaiStdDev_500m\",\"Lai_500m\"])\n",
    "print(fileList)\n",
    "print(loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://d2v96idf0qj40l.cloudfront.net/s3-eab5ea0d219da2588ae2241aa105a663/appeears-output.s3.amazonaws.com/70c9cf66-2be0-4ee1-a668-c309fd5328f8/MOD15A2H-006-FparExtra-QC-Statistics-QA.csv?response-content-disposition=attachment%253B%2520filename%253DMOD15A2H-006-FparExtra-QC-Statistics-QA.csv&AWSAccessKeyId=ASIA3VC75OCO4EZCUFBW&Signature=bLZlJsmXiIX99mtl6JoWI2xnUME%253D&x-amz-security-token=IQoJb3JpZ2luX2VjEEMaCXVzLXdlc3QtMiJHMEUCIQC1b%252BMwpt6O%252FRA3NAj01rVbIfzDmxdYS%252Fv3JYZAxiQnVAIgGR67eK0adQJjzhl5ehCqaNQEBRJ4SRKOOAZep%252F%252FIjLUqiQQIHBAAGgw4MDEyMTI1NTk1MTciDK7dKSRkPRWux93O5yrmA3qInKb9uU%252BLCcDTfKUP6rVg1qNhlMRenRih0zrPwhDH9qDbzhnvI8mowT8ENMsjx6AndrL3YIcg7W42AbUF6HgbnINm4f4Z3BiA91p4T455%252FAKbzWG8w%252BW1VmSKeogBHFSGJoA3iHE4Owp3ON8Lu1Ln3AgWbTUQLqFvgRUXOySSDvB8MccQ10wWd0frOGfjLho5UvYsXHA8bZloAhnvfHUUeporBZLxZo%252F9ZX%252FazbFY3zk7fnHrdFNk87rNV0nYqwcTYxrfgX1pmbpZjxZxRRh28A7Nr3hQxkBBmnW8ihZv%252Bmb9LqcUGtQiLCeMnMcPucsFPWFe9q27sA8m2JgB85mCuxgZUkzXVkLBlkOHHu8Vx4bnaL6PGOZZlhr8JB2Hgvg%252BHxx8IaVt%252BKwSpcMB29%252FhSmZKROWZtagZ0aqHSLHQq7mvGYOyT8OM7qFruvPR9OTEzMnO3EfEiCUC40lY88GopR13ld7VE2dFNlYTDoQ0L1C0seszluGI7PPJvWZwBl5LmhN%252FtVZ6Bg0c76Gxg1KYoqj%252Fy4SUsVYduq%252FNqu%252BHdDUSx7lRUVFJu36yfCJiiHDSt8wNV8tbuSq8%252BVeUIt8Xswh3mLzX1TJt3VRE%252BOio6qPrAQmg0k9r1WzFjlqKT7KQlVh7TDDV3eqTBjqlAdZYhu%252FuAqsaeQGkgPRdHOzCri%252F0X51HS4k6DixDhk73q15V0nGGqz08sIhdVrDLLQiMhwXN7%252BHwv0KlF%252Bn%252FHkAv9csxS27ID%252BXcCnOxB54mRr5yh60%252BbqA8MwlWvdq7iNCNXZr9EiltpXG%252B9pfkenul3f4aRuH0rMP2l%252FZ8nolIQW0ZPte3hm80Qt1ZzyDf6yZ4NZQNrzkxTBA9OmWKFgtQfyDbCg%253D%253D&Expires=1652387234'\n",
    "fail = pd.read_csv(url)\n",
    "fail['locId'] = 'L10129002'\n",
    "fail['latestUpdate'] = str(dt.datetime.today())\n",
    "fail.to_sql(name='MOD15A2H-006-FparExtra-QC-Statistics-QA.csv_MODIS_cooking',con=connectDB(),if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce2b8b10e8082f390f0f7c9c12f304c9df3ed4554edd4b21c0fcee2d9ef65582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
