{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json\n",
    "import geojson\n",
    "import requests\n",
    "import sqlite3 as db\n",
    "sys.path.append('../')\n",
    "\n",
    "pd.options.display.max_rows=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_userName = 'Tanag3r'\n",
    "ebird_token = 'j6c7l80ga2ib'\n",
    "db_name = 'trailheadDirectBirds_sous.db'\n",
    "##cur = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##connect to database\n",
    "def connectDB():\n",
    "    try:\n",
    "        cnx = db.connect(db_name)\n",
    "    except Exception as cnxError:\n",
    "        raise UserWarning(f'Unable to connect to database due to: {cnxError}')\n",
    "    return cnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Data Products:\n",
    "- LAI: LAI is a measure for the total area of leaves per unit ground area and directly related to the amount of light that can be intercepted by plants. It is defined as the one-sided green leaf area per unit ground surface area (LAI = leaf area / ground area, m2 / m2) in broadleaf canopies. There are three methods used to measure LAI for conifers; this project uses projected (or one-sided, in accordance the definition for broadleaf canopies) needle area per unit ground area.\n",
    "    - In general, a higher LAI value indicates more leaf coverage\n",
    "- fPAR: Fraction of absorbed photosynthetically active radiation (fPAR) is the fraction of incoming solar radiation in the spectrum of 400â€“700 nm that is absorbed by vegetation canopy. Data is provided as a percentage.\n",
    "- Land Cover Type 3: Annual Leaf Area Index (LAI) classification\n",
    "- FAO-Land Cover Classification System 1 (LCCS1) land cover layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #40 remove the getpass prompt and replace with a credential manager call\n",
    "earthdata_baseUrl = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'\n",
    "appEEARS_username = 'lwylie'\n",
    "appEEARS_password = 'BdiUPBhUa7ma5ds'\n",
    "import getpass\n",
    "NASA_username = getpass.getpass(prompt = 'Enter NASA Earthdata Login Username: ')\n",
    "NASA_password = getpass.getpass(prompt = 'Enter NASA Earthdata Login Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_type': 'Bearer', 'token': 'UK3jN04-qCuoyKBYRHqVEYjdzdCzjQrYueviDTdCd4WlCHx0N955kRGFERJrjsCvqsqtJftAXF_TQiKx1OGaTw', 'expiration': '2022-02-27T20:34:16Z'}\n"
     ]
    }
   ],
   "source": [
    "##obtain an Earthdata token\n",
    "##TODO #36 the earthdata API is often under maintenance, write a script to abort this process if a new token cannot be obtain\n",
    "##TODO #56 refactor block into a function that requests a token from the earthdata service and returns the header on success\n",
    "earthdata_loginURL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/login'\n",
    "earthdata_loginRequest = requests.post(earthdata_loginURL,auth=(NASA_username,NASA_password))\n",
    "earthdata_loginResponse = earthdata_loginRequest.json()\n",
    "print(earthdata_loginResponse)\n",
    "##Transcribe token, builder header\n",
    "earthdata_token = earthdata_loginResponse['token']\n",
    "earthdata_head = {'Authorization': 'Bearer {}'.format(earthdata_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the earthdata dictionary from the database and return a list of locId's without Prop1 and Type3 data\n",
    "def list_needMODIS():\n",
    "    cnx = connectDB()\n",
    "    diff = 0.005    #equivalent to half a mile\n",
    "    try:\n",
    "        #dataframe of hotspots from the database\n",
    "        hotspotsGeo = pd.read_sql('SELECT locId,lat,lng FROM Hotspots WHERE locId not in (SELECT DISTINCT locId from earthdata_dictionary)', con=cnx)\n",
    "        if hotspotsGeo.empty == True:\n",
    "            raise Exception(f'An empty dataframe has been returned')\n",
    "        hotspotsGeo.sort_values(by=['locId'],ascending=True,inplace=True)\n",
    "        hotspotsGeo.drop_duplicates(subset=['locId'],keep='first',inplace=True)\n",
    "        hotspotsGeo.reset_index()\n",
    "        #build out squares around each hotspot\n",
    "        def NW(x,y):\n",
    "            return x-diff,y+diff\n",
    "        def NE(x,y):\n",
    "            return x+diff,y+diff\n",
    "        def SE(x,y):\n",
    "            return x+diff,y-diff\n",
    "        def SW(x,y):\n",
    "            return x-diff,y-diff\n",
    "        ##apply the functions as new columns\n",
    "        ##NOTE that appEEARS only accepts coordinates as (longitude,latitude) which is contrary to geoJSON documentation\n",
    "        hotspotsGeo['NW'] = hotspotsGeo.apply(lambda i: NW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "        hotspotsGeo['NE'] = hotspotsGeo.apply(lambda i: NE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "        hotspotsGeo['SE'] = hotspotsGeo.apply(lambda i: SE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "        hotspotsGeo['SW'] = hotspotsGeo.apply(lambda i: SW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "    \n",
    "    except Exception as exd:\n",
    "        raise UserWarning(f'An unexpected error occurred in the function list_needMODIS: {exd}')\n",
    "    return hotspotsGeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "1. Make all requests in a loop, producing a list of {'locId': 'earthdata_taskID'}\n",
    "2. For each locId, pull each .csv into a dataframe then load that dataframe into a database table bearing the name that corresponds with the layer and product. Append the locId.\n",
    "    EXAMPLE: the contents of the .csv file for 'MCD12Q1-006-LC-Prop1-Statistics.csv' go into the table 'MCD12Q1-006-LC-Prop1-Statistics' in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locId</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>NW</th>\n",
       "      <th>NE</th>\n",
       "      <th>SE</th>\n",
       "      <th>SW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>L1855923</td>\n",
       "      <td>47.683891</td>\n",
       "      <td>-122.400562</td>\n",
       "      <td>(-122.40556219999999, 47.688891000000005)</td>\n",
       "      <td>(-122.3955622, 47.688891000000005)</td>\n",
       "      <td>(-122.3955622, 47.678891)</td>\n",
       "      <td>(-122.40556219999999, 47.678891)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>L1875789</td>\n",
       "      <td>47.629107</td>\n",
       "      <td>-122.391428</td>\n",
       "      <td>(-122.39642839999999, 47.6341071)</td>\n",
       "      <td>(-122.3864284, 47.6341071)</td>\n",
       "      <td>(-122.3864284, 47.624107099999996)</td>\n",
       "      <td>(-122.39642839999999, 47.624107099999996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>L1884056</td>\n",
       "      <td>47.644911</td>\n",
       "      <td>-122.365597</td>\n",
       "      <td>(-122.3705969, 47.6499113)</td>\n",
       "      <td>(-122.3605969, 47.6499113)</td>\n",
       "      <td>(-122.3605969, 47.639911299999994)</td>\n",
       "      <td>(-122.3705969, 47.639911299999994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>L1924233</td>\n",
       "      <td>47.655213</td>\n",
       "      <td>-122.414131</td>\n",
       "      <td>(-122.4191312, 47.660213000000006)</td>\n",
       "      <td>(-122.4091312, 47.660213000000006)</td>\n",
       "      <td>(-122.4091312, 47.650213)</td>\n",
       "      <td>(-122.4191312, 47.650213)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>L208659</td>\n",
       "      <td>47.665358</td>\n",
       "      <td>-122.397465</td>\n",
       "      <td>(-122.4024647, 47.6703579)</td>\n",
       "      <td>(-122.3924647, 47.6703579)</td>\n",
       "      <td>(-122.3924647, 47.660357899999994)</td>\n",
       "      <td>(-122.4024647, 47.660357899999994)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       locId        lat         lng  \\\n",
       "57  L1855923  47.683891 -122.400562   \n",
       "64  L1875789  47.629107 -122.391428   \n",
       "71  L1884056  47.644911 -122.365597   \n",
       "61  L1924233  47.655213 -122.414131   \n",
       "67   L208659  47.665358 -122.397465   \n",
       "\n",
       "                                           NW  \\\n",
       "57  (-122.40556219999999, 47.688891000000005)   \n",
       "64          (-122.39642839999999, 47.6341071)   \n",
       "71                 (-122.3705969, 47.6499113)   \n",
       "61         (-122.4191312, 47.660213000000006)   \n",
       "67                 (-122.4024647, 47.6703579)   \n",
       "\n",
       "                                    NE                                  SE  \\\n",
       "57  (-122.3955622, 47.688891000000005)           (-122.3955622, 47.678891)   \n",
       "64          (-122.3864284, 47.6341071)  (-122.3864284, 47.624107099999996)   \n",
       "71          (-122.3605969, 47.6499113)  (-122.3605969, 47.639911299999994)   \n",
       "61  (-122.4091312, 47.660213000000006)           (-122.4091312, 47.650213)   \n",
       "67          (-122.3924647, 47.6703579)  (-122.3924647, 47.660357899999994)   \n",
       "\n",
       "                                           SW  \n",
       "57           (-122.40556219999999, 47.678891)  \n",
       "64  (-122.39642839999999, 47.624107099999996)  \n",
       "71         (-122.3705969, 47.639911299999994)  \n",
       "61                  (-122.4191312, 47.650213)  \n",
       "67         (-122.4024647, 47.660357899999994)  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##small frame of hotspots for testing\n",
    "hotspotsGeo_short = pd.DataFrame(list_needMODIS()).head()\n",
    "hotspotsGeo_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoPack(NW,NE,SE,SW):\n",
    "    try:\n",
    "        geoPack_wrap = {\"type\": \"FeatureCollection\", \"features\":\n",
    "            [{\n",
    "            \"type\":\"Feature\",\n",
    "                \"geometry\":\n",
    "                    {\"type\": \"Polygon\",\n",
    "                    \"coordinates\":\n",
    "                        [[NW,NE,SE,SW,NW]]\n",
    "                    },\n",
    "                \"properties\": {}}]\n",
    "            }\n",
    "    except geojson.GeoJSON(geoPack_wrap).is_valid == False:\n",
    "        print(geojson.GeoJSON(geoPack_wrap).errors)\n",
    "    return geojson.GeoJSON(geoPack_wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(taskName: str,endDate: str,startDate: str,recurring: bool,yearRange: list, geoPack_wrap = geojson.GeoJSON):\n",
    "    try:\n",
    "        edTask = {\n",
    "            'task_type': 'area',\n",
    "            'task_name': taskName,\n",
    "            'params': {\n",
    "                'dates': \n",
    "                    [{\"endDate\": endDate, \n",
    "                    \"recurring\": recurring, \n",
    "                    \"startDate\": startDate, \n",
    "                    \"yearRange\": yearRange}],\n",
    "                'layers': \n",
    "                    [{\"layer\": \"LC_Prop1\", \"product\": \"MCD12Q1.006\"}, \n",
    "                    {\"layer\": \"LC_Type3\", \"product\": \"MCD12Q1.006\"}],\n",
    "                'output': {\n",
    "                    'format': {\n",
    "                        'type': 'netcdf4'}, \n",
    "                        'projection': 'geographic'},\n",
    "            'geo':geoPack_wrap}}\n",
    "    except Exception as JSONerror:\n",
    "        raise UserWarning(JSONerror)\n",
    "    return edTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##error logger function\n",
    "def log_earthdataError(message,statusCode: int,request: str,reqDate):\n",
    "    cnx = connectDB()\n",
    "    cur = cnx.cursor()\n",
    "    try:\n",
    "        sqliteInsert = 'INSERT INTO earthdata_errorlog (message,statusCode,request,reqDate) VALUES (?, ?, ?, ?)'\n",
    "        logTuple = (message,statusCode,request,reqDate)\n",
    "        cur.execute(sqliteInsert,logTuple)\n",
    "        cur.close()\n",
    "    except db.Error as sqlError:\n",
    "        raise UserWarning(sqlError)\n",
    "    finally:\n",
    "        if cnx:\n",
    "            cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #45 once all hotspots/locId's have Prop1 and Type3 MCD12Q1 data, convert this block to check for LAI and fPAR\n",
    "def post_earthdataTask():\n",
    "    import time\n",
    "    hotspotsGeo = hotspotsGeo_short    #small frame of hotspots for testing\n",
    "    #hotspotsGeo = pd.DataFrame(list_needMODIS()).head()\n",
    "    earthdataTaskList = []\n",
    "    try:\n",
    "        for locId in hotspotsGeo.itertuples():\n",
    "            time.sleep(0.5)\n",
    "            coords = geoPack(NW = locId.NW,NE = locId.NE,SE = locId.SE,SW = locId.SW)\n",
    "            earthdataTask = task(taskName = locId.locId,endDate = \"12-31\",startDate=\"01-01\",recurring=True,yearRange=[2017,2019],geoPack_wrap=coords)\n",
    "            taskReq = requests.post(f'{earthdata_baseUrl}task',json=earthdataTask,headers=earthdata_head)\n",
    "            if taskReq.status_code != 200:\n",
    "                log_earthdataError(str(taskReq),int(taskReq.status_code),json.dumps(earthdataTask),dt.datetime.today())\n",
    "            earthdataTaskList.append(taskReq.json())\n",
    "    except Exception as ee:\n",
    "        raise UserWarning(ee)\n",
    "    return earthdataTaskList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': 'b92875d9-17d2-4a55-9e51-c02bc51e5e2c', 'status': 'pending'},\n",
       " {'task_id': '2dcbfb81-95a0-4360-b8c9-368074dc6d36', 'status': 'pending'},\n",
       " {'task_id': '082100a5-59f1-4c7c-9d33-00406b41bf34', 'status': 'pending'},\n",
       " {'task_id': 'e4013ebc-8767-47ce-9c16-c44e31acd06b', 'status': 'pending'},\n",
       " {'task_id': '544a7546-295b-43d6-bcc6-3145145c43ec', 'status': 'pending'}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help = post_earthdataTask()\n",
    "help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #55 test implementation of list_needMODIS() within get_earthdataTask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earthdataTask():\n",
    "    postedTasks = pd.DataFrame(help) #for testing\n",
    "    taskInfo = []\n",
    "    fileList = []\n",
    "    cnx = connectDB()\n",
    "    ##postedTasks = pd.DataFrame(post_earthdata_Task())\n",
    "    import time\n",
    "    try:\n",
    "        for task_id in postedTasks.itertuples():\n",
    "            while requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()['status'] !='done':\n",
    "                requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()['status']\n",
    "                time.sleep(10.0)\n",
    "            taskStatus = requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()\n",
    "            taskInfo.append(taskStatus)\n",
    "        taskBatch = pd.DataFrame(data=taskInfo,columns=['task_id','task_name','status','completed'])\n",
    "\n",
    "        for task_id in taskBatch.itertuples():\n",
    "            earthdata_bundle = requests.get(f'{earthdata_baseUrl}bundle/{task_id.task_id}').json()\n",
    "            ##filter down to .csv files\n",
    "            for x in earthdata_bundle['files']:\n",
    "                if x['file_type'] in 'csv':\n",
    "                    time.sleep(0.3)\n",
    "                    fileID = x['file_id']\n",
    "                    fileName = x['file_name']\n",
    "                    ##To read results to a dataframe, pass in earthdata_baseUrl + 'bundle/ + taskID/ + file_id/ + file_name\n",
    "                    getIt = pd.DataFrame(pd.read_csv(f'{earthdata_baseUrl}bundle/{task_id.task_id}/{fileID}/{fileName}'))\n",
    "                    getIt['locId'] = task_id.task_name\n",
    "                    try:\n",
    "                        getIt.to_sql(name=f'{fileName}_MODIS_cooking',con=cnx,if_exists='append')\n",
    "                    ##TODO #53 test the exception\n",
    "                    except db.DatabaseError as resultError:\n",
    "                        fileList.append({resultError: resultError.__cause__,task_id.task_name: f'{earthdata_baseUrl}bundle/{task_id.task_id}/{fileID}/{fileName}'})\n",
    "                        pass\n",
    "                else: continue  \n",
    "    except Exception as ex:\n",
    "        raise UserWarning(ex)\n",
    "    cnx.close()\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukew\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2872: UserWarning: The spaces in these column names will not be changed. In pandas versions < 0.14, spaces were converted to underscores.\n",
      "  sql.to_sql(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{sqlite3.OperationalError('table MCD12Q1-006-QC-Statistics-QA.csv_MODIS_cooking has no column named 4'): None,\n",
       "  'L128530': 'https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/082100a5-59f1-4c7c-9d33-00406b41bf34/0730af6a-adf6-4c42-8771-c535a5b116e4/MCD12Q1-006-QC-Statistics-QA.csv'},\n",
       " {sqlite3.OperationalError('table MCD12Q1-006-LC-Prop1-Statistics.csv_MODIS_cooking has no column named (3) Water Bodies'): None,\n",
       "  'L128530': 'https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/082100a5-59f1-4c7c-9d33-00406b41bf34/8d568066-48ff-4390-a91d-08a16f66a3b5/MCD12Q1-006-LC-Prop1-Statistics.csv'},\n",
       " {sqlite3.OperationalError('table MCD12Q1-006-LC-Type3-Statistics.csv_MODIS_cooking has no column named (0) Water Bodies'): None,\n",
       "  'L128530': 'https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/082100a5-59f1-4c7c-9d33-00406b41bf34/b89870fa-9729-4e80-90bb-d811e31c2a20/MCD12Q1-006-LC-Type3-Statistics.csv'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg = get_earthdataTask()\n",
    "tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lxx = pd.read_csv('https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/082100a5-59f1-4c7c-9d33-00406b41bf34/b89870fa-9729-4e80-90bb-d811e31c2a20/MCD12Q1-006-LC-Type3-Statistics.csv')\n",
    "lxx['locId'] = 'L128530'\n",
    "lxx.to_sql(name='MCD12Q1-006-LC-Type3-Statistics.csv_MODIS_cooking',con=connectDB(),if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce2b8b10e8082f390f0f7c9c12f304c9df3ed4554edd4b21c0fcee2d9ef65582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
