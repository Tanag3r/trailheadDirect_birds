{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json\n",
    "import geojson\n",
    "import requests\n",
    "import sqlite3 as db\n",
    "sys.path.append('../')\n",
    "\n",
    "pd.options.display.max_rows=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_userName = 'Tanag3r'\n",
    "ebird_token = 'j6c7l80ga2ib'\n",
    "db_name = 'trailheadDirectBirds_sous.db'\n",
    "##cur = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##connect to database\n",
    "def connectDB():\n",
    "    try:\n",
    "        cnx = db.connect(db_name)\n",
    "    except Exception as cnxError:\n",
    "        raise UserWarning(f'Unable to connect to database due to: {cnxError}')\n",
    "    return cnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Data Products:\n",
    "- LAI: LAI is a measure for the total area of leaves per unit ground area and directly related to the amount of light that can be intercepted by plants. It is defined as the one-sided green leaf area per unit ground surface area (LAI = leaf area / ground area, m2 / m2) in broadleaf canopies. There are three methods used to measure LAI for conifers; this project uses projected (or one-sided, in accordance the definition for broadleaf canopies) needle area per unit ground area.\n",
    "    - In general, a higher LAI value indicates more leaf coverage\n",
    "- fPAR: Fraction of absorbed photosynthetically active radiation (fPAR) is the fraction of incoming solar radiation in the spectrum of 400â€“700 nm that is absorbed by vegetation canopy. Data is provided as a percentage.\n",
    "- Land Cover Type 3: Annual Leaf Area Index (LAI) classification\n",
    "- FAO-Land Cover Classification System 1 (LCCS1) land cover layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = 0.005    ##equivalent to 0.5 miles when applied\n",
    "hotspotsGeo = pd.DataFrame()\n",
    "hotspotsGeo = pd.read_sql('SELECT locId,lat,lng FROM Hotspots', con=connectDB())\n",
    "##TODO #34 remove duplicate hotspots before building table of polygons for each hotspot --DONE\n",
    "##hotspotsGeo.set_index('index',inplace=True)\n",
    "hotspotsGeo.sort_values(by=['locId'],ascending=True,inplace=True)\n",
    "hotspotsGeo.drop_duplicates(subset=['locId'],keep='first',inplace=True)\n",
    "hotspotsGeo.reset_index()\n",
    "##TODO #33 write a function to increment the lat & lng values as needed then output the results to a single column in the format lat,lng --DONE\n",
    "\n",
    "##Define functions to produce a corner of a polygon for each hotspot (NE,SE,SW,NW)\n",
    "def NW(x,y):\n",
    "    return x-diff,y+diff\n",
    "def NE(x,y):\n",
    "    return x+diff,y+diff\n",
    "def SE(x,y):\n",
    "    return x+diff,y-diff\n",
    "def SW(x,y):\n",
    "    return x-diff,y-diff\n",
    "\n",
    "##apply the functions as new columns\n",
    "##NOTE that appEEARS only accepts coordinates as (longitude,latitude) which is contrary to geoJSON documentation\n",
    "hotspotsGeo['NW'] = hotspotsGeo.apply(lambda i: NW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['NE'] = hotspotsGeo.apply(lambda i: NE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['SE'] = hotspotsGeo.apply(lambda i: SE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['SW'] = hotspotsGeo.apply(lambda i: SW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "##TODO #35 (script to support) write the results to the database as a new table with a 'latestUpdate' column --DONE\n",
    "today = dt.datetime.today()\n",
    "hotspotsGeo['latestUpdate'] = today.date()\n",
    "hotspotsGeo['latestUpdate'] = hotspotsGeo['latestUpdate'].astype('datetime64[ns]')\n",
    "##hotspotsGeo.to_sql(name='hotspotsGeo',con=cnx,if_exists='append')\n",
    "\n",
    "##TODO #37 compile the polygon generated for each hotspot into a coordinate pack formatted for JSON insert --DONE\n",
    "##no longer neccessary, geoJSON handles packaging\n",
    "##for locId in hotspotsGeo.itertuples():\n",
    "    ##hotspotsGeo['polygon'] = '[[{},{},{},{},{}]]'.format(locId.NW,locId.NE,locId.SE,locId.SW,locId.NW).replace('(','[').replace(')',']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #40 remove the getpass prompt and replace with a credential manager call\n",
    "earthdata_baseUrl = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'\n",
    "appEEARS_username = 'lwylie'\n",
    "appEEARS_password = 'BdiUPBhUa7ma5ds'\n",
    "import getpass\n",
    "NASA_username = getpass.getpass(prompt = 'Enter NASA Earthdata Login Username: ')\n",
    "NASA_password = getpass.getpass(prompt = 'Enter NASA Earthdata Login Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_type': 'Bearer', 'token': '_XZ-yiaeVWRoKjWtV8d2PpIeqe54GcjjvxGOebN0nnlC7rP4X88SOiCAej_XWHcrAxRO38lCgEZNkOyidkwnZQ', 'expiration': '2022-02-19T18:00:30Z'}\n"
     ]
    }
   ],
   "source": [
    "##obtain an Earthdata token\n",
    "##TODO #36 the earthdata API is often under maintenance, write a script to abort this process if a new token cannot be obtain\n",
    "earthdata_loginURL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/login'\n",
    "earthdata_loginRequest = requests.post(earthdata_loginURL,auth=(NASA_username,NASA_password))\n",
    "earthdata_loginResponse = earthdata_loginRequest.json()\n",
    "print(earthdata_loginResponse)\n",
    "##Transcribe token, builder header\n",
    "earthdata_token = earthdata_loginResponse['token']\n",
    "earthdata_head = {'Authorization': 'Bearer {}'.format(earthdata_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #43 write a cooked table of locId's with stats to check against list of hotspots. Drive fetch off difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "1. Make all requests in a loop, producing a list of {'locId': 'earthdata_taskID'}\n",
    "2. For each locId, pull each .csv into a dataframe then load that dataframe into a database table bearing the name that corresponds with the layer and product. Append the locId.\n",
    "    EXAMPLE: the contents of the .csv file for 'MCD12Q1-006-LC-Prop1-Statistics.csv' go into the table 'MCD12Q1-006-LC-Prop1-Statistics' in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##small frame of hotspots for testing\n",
    "hotspotsGeo_short = hotspotsGeo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoPack(NW,NE,SE,SW):\n",
    "    geoPack_wrap = {\"type\": \"FeatureCollection\", \"features\":\n",
    "        [{\n",
    "        \"type\":\"Feature\",\n",
    "            \"geometry\":\n",
    "                {\"type\": \"Polygon\",\n",
    "                \"coordinates\":\n",
    "                    [[NW,NE,SE,SW,NW]]\n",
    "                },\n",
    "            \"properties\": {}}]\n",
    "        }\n",
    "    return geojson.GeoJSON(geoPack_wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(taskName: str,endDate: str,startDate: str,recurring: bool,yearRange: list, geoPack_wrap = geojson.GeoJSON):\n",
    "    \n",
    "    edTask = {\n",
    "        'task_type': 'area',\n",
    "        'task_name': taskName,\n",
    "        'params': {\n",
    "            'dates': \n",
    "                [{\"endDate\": endDate, \n",
    "                \"recurring\": recurring, \n",
    "                \"startDate\": startDate, \n",
    "                \"yearRange\": yearRange}],\n",
    "            'layers': \n",
    "                [{\"layer\": \"LC_Prop1\", \"product\": \"MCD12Q1.006\"}, \n",
    "                {\"layer\": \"LC_Type3\", \"product\": \"MCD12Q1.006\"}],\n",
    "            'output': {\n",
    "                'format': {\n",
    "                    'type': 'netcdf4'}, \n",
    "                    'projection': 'geographic'},\n",
    "        'geo':geoPack_wrap}}\n",
    "    return edTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_earthdataTask():\n",
    "    earthdataTaskList = []\n",
    "    try:\n",
    "        for locId in hotspotsGeo_short.itertuples():\n",
    "            coords = geoPack(NW = locId.NW,NE = locId.NE,SE = locId.SE,SW = locId.SW)\n",
    "            earthdataTask = task(taskName = locId.locId,endDate = \"12-31\",startDate=\"01-01\",recurring=True,yearRange=[2017,2019],geoPack_wrap=coords)\n",
    "            taskReq = requests.post(f'{earthdata_baseUrl}task',json=earthdataTask,headers=earthdata_head).json()\n",
    "            earthdataTaskList.append(taskReq)\n",
    "    except Exception as ee:\n",
    "        raise UserWarning(ee)\n",
    "    return earthdataTaskList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "help = post_earthdataTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'task_id': '48f1b070-2dd8-47f1-b7dd-3003f05ecc40', 'status': 'pending'},\n",
       " {'task_id': 'd4d0be89-3b6b-454b-a9bb-f2a5bfc6c98e', 'status': 'pending'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #50 write a function that splits out bad responses from appEEARS and logs them to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #44 resolve error: {'message': \"RecursionError('maximum recursion depth exceeded in __instancecheck__',)\"} --DONE\n",
    "##TODO #45 once all hotspots/locId's have Prop1 and Type3 MCD12Q1 data, convert this block to check for LAI and fPAR\n",
    "\n",
    "    \n",
    "##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log --DONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskid = pd.DataFrame(help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##submit the task request\n",
    "##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log\n",
    "params = {'limit': 15, 'pretty': True}\n",
    "earthdata_tasks = requests.get('{}task'.format(earthdata_baseUrl),params=params,headers=earthdata_head).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #51 write a function that takes the results of get_earthdataTask(), fetches the .csv, returns a DataFrame and then pushes that DataFrame to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earthdataTask():\n",
    "    postedTasks = pd.DataFrame(help)\n",
    "    taskInfo = []\n",
    "    cnx = connectDB()\n",
    "    ##postedTasks = pd.DataFrame(post_earthdata_Task())\n",
    "    import time\n",
    "    try:\n",
    "        for task_id in postedTasks.itertuples():\n",
    "            taskStatus = requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()\n",
    "            starttime = time.time()\n",
    "            while taskStatus['status'] !='done':\n",
    "                print(requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()['status'])\n",
    "                time.sleep(20.0 - ((time.time() - starttime) % 20.0))\n",
    "            taskInfo.append(taskStatus)\n",
    "        taskBatch = pd.DataFrame(data=taskInfo,columns=['task_id','task_name','status','completed'])\n",
    "\n",
    "        for task_id in taskBatch.itertuples():\n",
    "            earthdata_bundle = requests.get(f'{earthdata_baseUrl}bundle/{task_id.task_id}').json()\n",
    "            ##filter down to .csv files\n",
    "            fileList = []\n",
    "            for x in earthdata_bundle['files']:\n",
    "                if x['file_type'] in 'csv':\n",
    "                    time.sleep(0.3)\n",
    "                    fileID = x['file_id']\n",
    "                    fileName = x['file_name']\n",
    "                    ##To read results to a dataframe, pass in earthdata_baseUrl + 'bundle/ + taskID/ + file_id/ + file_name\n",
    "                    getIt = pd.DataFrame(pd.read_csv(f'{earthdata_baseUrl}bundle/{task_id.task_id}/{fileID}/{fileName}'))\n",
    "                    ##TODO #52 almost there, make this script below function:\n",
    "                    ##getIt = pd.read_csv(f'{earthdata_baseUrl}bundle/{task_id.task_id}/{fileID}/{fileName}')\n",
    "                    ##getIt.to_sql(name='MODIS_cooking',con=cnx,if_exists='append')\n",
    "                    \n",
    "                else: continue\n",
    "                allgets = pd.concat([getIt])     \n",
    "    except Exception as ex:\n",
    "        raise UserWarning(ex)\n",
    "    return allgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>aid</th>\n",
       "      <th>Date</th>\n",
       "      <th>(7) Evergreen Needleleaf Forests</th>\n",
       "      <th>(10) Urban and Built-up Lands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LC_Type3_2017001_aid0001</td>\n",
       "      <td>aid0001</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LC_Type3_2018001_aid0001</td>\n",
       "      <td>aid0001</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC_Type3_2019001_aid0001</td>\n",
       "      <td>aid0001</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File      aid        Date  \\\n",
       "0  LC_Type3_2017001_aid0001  aid0001  2017-01-01   \n",
       "1  LC_Type3_2018001_aid0001  aid0001  2018-01-01   \n",
       "2  LC_Type3_2019001_aid0001  aid0001  2019-01-01   \n",
       "\n",
       "   (7) Evergreen Needleleaf Forests  (10) Urban and Built-up Lands  \n",
       "0                                 2                              7  \n",
       "1                                 2                              7  \n",
       "2                                 2                              7  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = get_earthdataTask()\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'files': [{'sha256': '58fb0e19dfbd85ad0f636fa525b44ffa272bc212371dd74c42626f1823681fb7',\n",
       "   'file_id': 'ca0e52f7-3f80-4777-8b7c-507d09f245dd',\n",
       "   'file_name': 'MCD12Q1.006_500m_aid0001.nc',\n",
       "   'file_size': 35199,\n",
       "   'file_type': 'nc'},\n",
       "  {'sha256': 'd29d9a15144007340d7b2df860d3b1d9226f1eb645b2df3527b720722ceec1f4',\n",
       "   'file_id': 'b1e45e8d-1548-4993-bb5f-f9e46ed7fc29',\n",
       "   'file_name': 'MCD12Q1-006-QC-lookup.csv',\n",
       "   'file_size': 99,\n",
       "   'file_type': 'csv'},\n",
       "  {'sha256': 'ed3ab9b81e7c0b98695bc53839cd4ec2cef4b78ab23cf287232d82d53b6e72a5',\n",
       "   'file_id': 'e6ab2e68-916b-4866-a260-da30730af562',\n",
       "   'file_name': 'MCD12Q1-006-QC-Statistics-QA.csv',\n",
       "   'file_size': 136,\n",
       "   'file_type': 'csv'},\n",
       "  {'sha256': '82fa1a7931a7f04ad1254e1427f6ee271bca68b66fa362267abd609c7194926d',\n",
       "   'file_id': 'f9c5ec31-096e-48be-b0e5-f9a2bbe963b7',\n",
       "   'file_name': 'MCD12Q1-006-LC-Prop1-Statistics.csv',\n",
       "   'file_size': 235,\n",
       "   'file_type': 'csv'},\n",
       "  {'sha256': '79b145aed0fc5a10f4d65f69af9959b88900189c34605e0e2312ddb9de96c31f',\n",
       "   'file_id': '6309a3d1-ac26-498d-bee2-03d03f2bbe09',\n",
       "   'file_name': 'MCD12Q1-006-LC-Type3-Statistics.csv',\n",
       "   'file_size': 221,\n",
       "   'file_type': 'csv'},\n",
       "  {'sha256': 'a4f3498d9a417fe1c9e721a98652ec9b02b53b0765424aa1d3bb75a7b4f54305',\n",
       "   'file_id': '701338ee-6b73-406d-851b-4c0c00297c6f',\n",
       "   'file_name': 'L10128988-granule-list.txt',\n",
       "   'file_size': 345,\n",
       "   'file_type': 'txt'},\n",
       "  {'sha256': '1ccad2d08d6520622da0ce6869c41ad6f0fe0a1470424aa7ea54f96d89af3a72',\n",
       "   'file_id': 'a7e431e5-d727-4def-9976-bfd50c6f1eb9',\n",
       "   'file_name': 'L10128988-request.json',\n",
       "   'file_size': 1123,\n",
       "   'file_type': 'json'},\n",
       "  {'sha256': '5769be989b00dc2b918833d62eec98dfea762c54193e9fdb46f7ba15ff006543',\n",
       "   'file_id': '0f3d8199-c786-40ac-a99b-9ba7a832690b',\n",
       "   'file_name': 'L10128988-MCD12Q1-006-metadata.xml',\n",
       "   'file_size': 22059,\n",
       "   'file_type': 'xml'},\n",
       "  {'sha256': 'd25e99f7f718c681126be04f874abb0f4d5d8ee2af7b55eabf08c8235adff580',\n",
       "   'file_id': 'd68e717c-9359-4a49-8727-1fce2bf4aee2',\n",
       "   'file_name': 'README.md',\n",
       "   'file_size': 23396,\n",
       "   'file_type': 'txt'}],\n",
       " 'created': '2022-02-17T20:23:53.995832',\n",
       " 'task_id': '48f1b070-2dd8-47f1-b7dd-3003f05ecc40',\n",
       " 'updated': '2022-02-17T20:24:01.473985',\n",
       " 'bundle_type': 'area'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO #38 get the results of the appEEARS task as a bundle\n",
    "earthdata_taskID = '48f1b070-2dd8-47f1-b7dd-3003f05ecc40'\n",
    "earthdata_bundle = requests.get('{}bundle/{}'.format(earthdata_baseUrl,earthdata_taskID),).json()\n",
    "earthdata_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12112/398618806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mearthdata_bundle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'files'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "earthdata_bundle['files']['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sha256': 'd29d9a15144007340d7b2df860d3b1d9226f1eb645b2df3527b720722ceec1f4',\n",
       "  'file_id': 'b1e45e8d-1548-4993-bb5f-f9e46ed7fc29',\n",
       "  'file_name': 'MCD12Q1-006-QC-lookup.csv',\n",
       "  'file_size': 99,\n",
       "  'file_type': 'csv'},\n",
       " {'sha256': 'ed3ab9b81e7c0b98695bc53839cd4ec2cef4b78ab23cf287232d82d53b6e72a5',\n",
       "  'file_id': 'e6ab2e68-916b-4866-a260-da30730af562',\n",
       "  'file_name': 'MCD12Q1-006-QC-Statistics-QA.csv',\n",
       "  'file_size': 136,\n",
       "  'file_type': 'csv'},\n",
       " {'sha256': '82fa1a7931a7f04ad1254e1427f6ee271bca68b66fa362267abd609c7194926d',\n",
       "  'file_id': 'f9c5ec31-096e-48be-b0e5-f9a2bbe963b7',\n",
       "  'file_name': 'MCD12Q1-006-LC-Prop1-Statistics.csv',\n",
       "  'file_size': 235,\n",
       "  'file_type': 'csv'},\n",
       " {'sha256': '79b145aed0fc5a10f4d65f69af9959b88900189c34605e0e2312ddb9de96c31f',\n",
       "  'file_id': '6309a3d1-ac26-498d-bee2-03d03f2bbe09',\n",
       "  'file_name': 'MCD12Q1-006-LC-Type3-Statistics.csv',\n",
       "  'file_size': 221,\n",
       "  'file_type': 'csv'}]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##filter for .csv's\n",
    "somelist = []\n",
    "earthdata_files = {}\n",
    "for f in earthdata_bundle['files']:\n",
    "    if f['file_type'] in 'csv':\n",
    "        somelist.append(f)\n",
    "    else: continue\n",
    "\n",
    "\n",
    "somelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##??\n",
    "for file_id in earthdata_files:\n",
    "    allresults = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IT WORKS\n",
    "##To read results to a dataframe, pass in earthdata_baseUrl + 'bundle/ + taskID/ + file_id/ + file_name\n",
    "##example: https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv\n",
    "qrf = pd.DataFrame()\n",
    "qrf = pd.read_csv('https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv')\n",
    "qrf.to_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##close the connection\n",
    "cnx.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce2b8b10e8082f390f0f7c9c12f304c9df3ed4554edd4b21c0fcee2d9ef65582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
