{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json\n",
    "import geojson\n",
    "import requests\n",
    "import sqlite3 as db\n",
    "sys.path.append('../')\n",
    "\n",
    "pd.options.display.max_rows=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_userName = 'Tanag3r'\n",
    "ebird_token = 'j6c7l80ga2ib'\n",
    "db_name = 'trailheadDirectBirds_sous.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##connect to database\n",
    "def connectDB():\n",
    "    try:\n",
    "        cnx = db.connect(db_name)\n",
    "    except Exception as cnxError:\n",
    "        raise UserWarning(f'Unable to connect to database due to: {cnxError}')\n",
    "    return cnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Data Products:\n",
    "- LAI: LAI is a measure for the total area of leaves per unit ground area and directly related to the amount of light that can be intercepted by plants. It is defined as the one-sided green leaf area per unit ground surface area (LAI = leaf area / ground area, m2 / m2) in broadleaf canopies. There are three methods used to measure LAI for conifers; this project uses projected (or one-sided, in accordance the definition for broadleaf canopies) needle area per unit ground area.\n",
    "    - In general, a higher LAI value indicates more leaf coverage\n",
    "- fPAR: Fraction of absorbed photosynthetically active radiation (fPAR) is the fraction of incoming solar radiation in the spectrum of 400â€“700 nm that is absorbed by vegetation canopy. Data is provided as a percentage.\n",
    "- Land Cover Type 3: Annual Leaf Area Index (LAI) classification\n",
    "- FAO-Land Cover Classification System 1 (LCCS1) land cover layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #40 remove the getpass prompt and replace with a credential manager call\n",
    "earthdata_baseUrl = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'\n",
    "appEEARS_username = 'lwylie'\n",
    "appEEARS_password = 'BdiUPBhUa7ma5ds'\n",
    "import getpass\n",
    "NASA_username = getpass.getpass(prompt = 'Enter NASA Earthdata Login Username: ')\n",
    "NASA_password = getpass.getpass(prompt = 'Enter NASA Earthdata Login Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23172/3902168836.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mearthdata_loginURL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://lpdaacsvc.cr.usgs.gov/appeears/api/login'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mearthdata_loginRequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mearthdata_loginURL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNASA_username\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNASA_password\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mearthdata_loginResponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mearthdata_loginRequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mearthdata_loginResponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m##Transcribe token, builder header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    908\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "##obtain an Earthdata token\n",
    "##TODO #36 the earthdata API is often under maintenance, write a script to abort this process if a new token cannot be obtain\n",
    "##TODO #56 refactor block into a function that requests a token from the earthdata service and returns the header on success\n",
    "earthdata_loginURL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/login'\n",
    "earthdata_loginRequest = requests.post(earthdata_loginURL,auth=(NASA_username,NASA_password))\n",
    "earthdata_loginResponse = earthdata_loginRequest.json()\n",
    "print(earthdata_loginResponse)\n",
    "##Transcribe token, builder header\n",
    "earthdata_token = earthdata_loginResponse['token']\n",
    "earthdata_head = {'Authorization': 'Bearer {}'.format(earthdata_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the earthdata dictionary from the database and return a list of locId's without Prop1 and Type3 data\n",
    "def list_needMODIS(testList: list):\n",
    "    cnx = connectDB()\n",
    "    diff = 0.005    #equivalent to half a mile\n",
    "    try:\n",
    "        queryList = []\n",
    "        for i in testList:\n",
    "            i = str(i)\n",
    "            queryList.append(i)\n",
    "        queryList = str(queryList).strip('[]')\n",
    "        #dataframe of hotspots from the database\n",
    "        hotspotsGeo = pd.read_sql(f'SELECT locId,lat,lng FROM Hotspots WHERE locId not in (SELECT DISTINCT locId from lpdaac_dictionary WHERE LPDAACkey in ({queryList}));', con=cnx)\n",
    "        if hotspotsGeo.empty == True:\n",
    "            raise Exception(f'An empty dataframe has been returned')\n",
    "        else:\n",
    "            hotspotsGeo.sort_values(by=['locId'],ascending=True,inplace=True)\n",
    "            hotspotsGeo.drop_duplicates(subset=['locId'],keep='first',inplace=True)\n",
    "            hotspotsGeo.reset_index()\n",
    "        #build out squares around each hotspot\n",
    "            def NW(x,y):\n",
    "                return x-diff,y+diff\n",
    "            def NE(x,y):\n",
    "                return x+diff,y+diff\n",
    "            def SE(x,y):\n",
    "                return x+diff,y-diff\n",
    "            def SW(x,y):\n",
    "                return x-diff,y-diff\n",
    "        ##apply the functions as new columns\n",
    "        ##NOTE that appEEARS only accepts coordinates as (longitude,latitude) which is contrary to geoJSON documentation\n",
    "            hotspotsGeo['NW'] = hotspotsGeo.apply(lambda i: NW(i.lng,i.lat), axis = 1)\n",
    "            hotspotsGeo['NE'] = hotspotsGeo.apply(lambda i: NE(i.lng,i.lat), axis = 1)\n",
    "            hotspotsGeo['SE'] = hotspotsGeo.apply(lambda i: SE(i.lng,i.lat), axis = 1)\n",
    "            hotspotsGeo['SW'] = hotspotsGeo.apply(lambda i: SW(i.lng,i.lat), axis = 1)\n",
    "    \n",
    "    except Exception as exd:\n",
    "        raise UserWarning(f'An unexpected error occurred in the function list_needMODIS: {exd}')\n",
    "    return hotspotsGeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "1. Make all requests in a loop, producing a list of {'locId': 'earthdata_taskID'}\n",
    "2. For each locId, pull each .csv into a dataframe then load that dataframe into a database table bearing the name that corresponds with the layer and product. Append the locId.\n",
    "    EXAMPLE: the contents of the .csv file for 'MCD12Q1-006-LC-Prop1-Statistics.csv' go into the table 'MCD12Q1-006-LC-Prop1-Statistics' in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #59 update input on geoPack function to take a list of coordinates\n",
    "def geoPack(NW,NE,SE,SW):\n",
    "    try:\n",
    "        geoPack_wrap = {\"type\": \"FeatureCollection\", \"features\":\n",
    "            [{\n",
    "            \"type\":\"Feature\",\n",
    "                \"geometry\":\n",
    "                    {\"type\": \"Polygon\",\n",
    "                    \"coordinates\":\n",
    "                        [[NW,NE,SE,SW,NW]]\n",
    "                    },\n",
    "                \"properties\": {}}]\n",
    "            }\n",
    "    except geojson.GeoJSON(geoPack_wrap).is_valid == False:\n",
    "        print(geojson.GeoJSON(geoPack_wrap).errors)\n",
    "    return geojson.GeoJSON(geoPack_wrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO #60 update 'task' function to include layers as an input\n",
    "def task(taskName: str,endDate: str,startDate: str,recurring: bool,yearRange: list, geoPack_wrap = geojson.GeoJSON):\n",
    "    try:\n",
    "        edTask = {\n",
    "            'task_type': 'area',\n",
    "            'task_name': taskName,\n",
    "            'params': {\n",
    "                'dates': \n",
    "                    [{\"endDate\": endDate, \n",
    "                    \"recurring\": recurring, \n",
    "                    \"startDate\": startDate, \n",
    "                    \"yearRange\": yearRange}],\n",
    "                'layers': \n",
    "                    [{\"layer\": \"FparExtra_QC\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"FparLai_QC\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"FparStdDev_500m\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"Fpar_500m\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"LaiStdDev_500m\", \"product\": \"MOD15A2H.006\"}, \n",
    "                    {\"layer\": \"Lai_500m\", \"product\": \"MOD15A2H.006\"}],\n",
    "                'output': {\n",
    "                    'format': {\n",
    "                        'type': 'netcdf4'}, \n",
    "                        'projection': 'geographic'},\n",
    "            'geo':geoPack_wrap}}\n",
    "    except Exception as JSONerror:\n",
    "        raise UserWarning(JSONerror)\n",
    "    return edTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##error logger function\n",
    "def log_earthdataError(message,statusCode: int,request: str,reqDate):\n",
    "    cnx = connectDB()\n",
    "    cur = cnx.cursor()\n",
    "    try:\n",
    "        sqliteInsert = 'INSERT INTO earthdata_errorlog (message,statusCode,request,reqDate) VALUES (?, ?, ?, ?)'\n",
    "        logTuple = (message,statusCode,request,reqDate)\n",
    "        cur.execute(sqliteInsert,logTuple)\n",
    "        cnx.commit()\n",
    "        cur.close()\n",
    "    except db.Error as sqlError:\n",
    "        raise UserWarning(sqlError)\n",
    "    finally:\n",
    "        if cnx:\n",
    "            cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #45 once all hotspots/locId's have Prop1 and Type3 MCD12Q1 data, convert this block to check for LAI and fPAR\n",
    "\n",
    "#the testList variable requires a str list of tests from the LPDAAC keys table\n",
    "def post_earthdataTask(testList: list):\n",
    "    import time\n",
    "    #hotspotsGeo = hotspotsGeo_short    #small frame of hotspots for testing\n",
    "    hotspotsGeo = pd.DataFrame(list_needMODIS(testList=testList)).head()\n",
    "    earthdataTaskList = []\n",
    "    try:\n",
    "        for locId in hotspotsGeo.itertuples():\n",
    "            time.sleep(0.5)\n",
    "            coords = geoPack(NW = locId.NW,NE = locId.NE,SE = locId.SE,SW = locId.SW)\n",
    "            earthdataTask = task(taskName = locId.locId,endDate = \"12-31\",startDate=\"01-01\",recurring=True,yearRange=[2019,2021],geoPack_wrap=coords)\n",
    "            taskReq = requests.post(f'{earthdata_baseUrl}task',json=earthdataTask,headers=earthdata_head)\n",
    "            if taskReq.status_code != 200:\n",
    "                log_earthdataError(str(taskReq),int(taskReq.status_code),json.dumps(earthdataTask),dt.datetime.today())\n",
    "            earthdataTaskList.append(taskReq.json())\n",
    "    except Exception as ee:\n",
    "        raise UserWarning(ee)\n",
    "    return earthdataTaskList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_earthdataTask():\n",
    "    #postedTasks = pd.DataFrame(help) #for testing\n",
    "    taskInfo = []\n",
    "    fileList = []\n",
    "    cnx = connectDB()\n",
    "    postedTasks = pd.DataFrame(post_earthdataTask())\n",
    "    import time\n",
    "    try:\n",
    "        for task_id in postedTasks.itertuples():\n",
    "            while requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()['status'] !='done':\n",
    "                requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()['status']\n",
    "                time.sleep(20.0)\n",
    "            taskStatus = requests.get(f'{earthdata_baseUrl}task/{task_id.task_id}',headers=earthdata_head).json()\n",
    "            taskInfo.append(taskStatus)\n",
    "        taskBatch = pd.DataFrame(data=taskInfo,columns=['task_id','task_name','status','completed'])\n",
    "\n",
    "        for task_id in taskBatch.itertuples():\n",
    "            earthdata_bundle = requests.get(f'{earthdata_baseUrl}bundle/{task_id.task_id}').json()\n",
    "            ##filter down to .csv files\n",
    "            for x in earthdata_bundle['files']:\n",
    "                if x['file_type'] in 'csv':\n",
    "                    time.sleep(0.3)\n",
    "                    fileID = x['file_id']\n",
    "                    fileName = x['file_name']\n",
    "                    ##To read results to a dataframe, pass in earthdata_baseUrl + 'bundle/ + taskID/ + file_id/ + file_name\n",
    "                    getIt = pd.DataFrame(pd.read_csv(f'{earthdata_baseUrl}bundle/{task_id.task_id}/{fileID}/{fileName}'))\n",
    "                    getIt['locId'] = task_id.task_name\n",
    "                    try:\n",
    "                        getIt.to_sql(name=f'{fileName}_MODIS_cooking',con=cnx,if_exists='append')\n",
    "                    ##TODO #53 test the exception --DONE\n",
    "                    except db.DatabaseError as resultError:\n",
    "                        fileList.append({resultError: resultError.__cause__,task_id.task_name: f'{earthdata_baseUrl}bundle/{task_id.task_id}/{fileID}/{fileName}'})\n",
    "                        pass\n",
    "                else: continue  \n",
    "    except Exception as ex:\n",
    "        raise UserWarning(ex)\n",
    "    cnx.close()\n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg = get_earthdataTask()\n",
    "tg"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce2b8b10e8082f390f0f7c9c12f304c9df3ed4554edd4b21c0fcee2d9ef65582"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
