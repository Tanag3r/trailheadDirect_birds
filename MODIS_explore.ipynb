{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import json\n",
    "import geojson\n",
    "import requests\n",
    "import sqlite3 as db\n",
    "sys.path.append('../')\n",
    "\n",
    "pd.options.display.max_rows=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_userName = 'Tanag3r'\n",
    "ebird_token = 'j6c7l80ga2ib'\n",
    "cnx = db.connect('trailheadDirectBirds_sous.db')\n",
    "cur = cnx.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Data Products:\n",
    "- LAI: LAI is a measure for the total area of leaves per unit ground area and directly related to the amount of light that can be intercepted by plants. It is defined as the one-sided green leaf area per unit ground surface area (LAI = leaf area / ground area, m2 / m2) in broadleaf canopies. There are three methods used to measure LAI for conifers; this project uses projected (or one-sided, in accordance the definition for broadleaf canopies) needle area per unit ground area.\n",
    "    - In general, a higher LAI value indicates more leaf coverage\n",
    "- fPAR: Fraction of absorbed photosynthetically active radiation (fPAR) is the fraction of incoming solar radiation in the spectrum of 400â€“700 nm that is absorbed by vegetation canopy. Data is provided as a percentage.\n",
    "- Land Cover Type 3: Annual Leaf Area Index (LAI) classification\n",
    "- FAO-Land Cover Classification System 1 (LCCS1) land cover layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = 0.005    ##equivalent to 0.5 miles when applied\n",
    "hotspotsGeo = pd.DataFrame()\n",
    "hotspotsGeo = pd.read_sql('SELECT locId,lat,lng FROM Hotspots', con=cnx)\n",
    "##TODO #34 remove duplicate hotspots before building table of polygons for each hotspot --DONE\n",
    "##hotspotsGeo.set_index('index',inplace=True)\n",
    "hotspotsGeo.sort_values(by=['locId'],ascending=True,inplace=True)\n",
    "hotspotsGeo.drop_duplicates(subset=['locId'],keep='first',inplace=True)\n",
    "hotspotsGeo.reset_index()\n",
    "##TODO #33 write a function to increment the lat & lng values as needed then output the results to a single column in the format lat,lng --DONE\n",
    "\n",
    "##Define functions to produce a corner of a polygon for each hotspot (NE,SE,SW,NW)\n",
    "def NW(x,y):\n",
    "    return x-diff,y+diff\n",
    "def NE(x,y):\n",
    "    return x+diff,y+diff\n",
    "def SE(x,y):\n",
    "    return x+diff,y-diff\n",
    "def SW(x,y):\n",
    "    return x-diff,y-diff\n",
    "\n",
    "##apply the functions as new columns\n",
    "##NOTE that appEEARS only accepts coordinates as (longitude,latitude) which is contrary to geoJSON documentation\n",
    "hotspotsGeo['NW'] = hotspotsGeo.apply(lambda i: NW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['NE'] = hotspotsGeo.apply(lambda i: NE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['SE'] = hotspotsGeo.apply(lambda i: SE(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "hotspotsGeo['SW'] = hotspotsGeo.apply(lambda i: SW(i.lng,i.lat), axis = 1)##.astype(str)\n",
    "##TODO #35 (script to support) write the results to the database as a new table with a 'latestUpdate' column --DONE\n",
    "today = dt.datetime.today()\n",
    "hotspotsGeo['latestUpdate'] = today.date()\n",
    "hotspotsGeo['latestUpdate'] = hotspotsGeo['latestUpdate'].astype('datetime64[ns]')\n",
    "##hotspotsGeo.to_sql(name='hotspotsGeo',con=cnx,if_exists='append')\n",
    "\n",
    "##TODO #37 compile the polygon generated for each hotspot into a coordinate pack formatted for JSON insert --DONE\n",
    "##no longer neccessary, geoJSON handles packaging\n",
    "##for locId in hotspotsGeo.itertuples():\n",
    "    ##hotspotsGeo['polygon'] = '[[{},{},{},{},{}]]'.format(locId.NW,locId.NE,locId.SE,locId.SW,locId.NW).replace('(','[').replace(')',']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #40 remove the getpass prompt and replace with a credential manager call\n",
    "earthdata_baseUrl = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/'\n",
    "appEEARS_username = 'lwylie'\n",
    "appEEARS_password = 'BdiUPBhUa7ma5ds'\n",
    "import getpass\n",
    "NASA_username = getpass.getpass(prompt = 'Enter NASA Earthdata Login Username: ')\n",
    "NASA_password = getpass.getpass(prompt = 'Enter NASA Earthdata Login Password: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##obtain an Earthdata token\n",
    "##TODO #36 the earthdata API is often under maintenance, write a script to abort this process if a new token cannot be obtain\n",
    "earthdata_loginURL = 'https://lpdaacsvc.cr.usgs.gov/appeears/api/login'\n",
    "earthdata_loginRequest = requests.post(earthdata_loginURL,auth=(NASA_username,NASA_password))\n",
    "earthdata_loginResponse = earthdata_loginRequest.json()\n",
    "print(earthdata_loginResponse)\n",
    "##Transcribe token, builder header\n",
    "earthdata_token = earthdata_loginResponse['token']\n",
    "earthdata_head = {'Authorization': 'Bearer {}'.format(earthdata_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #43 write a cooked table of locId's with stats to check against list of hotspots. Drive fetch off difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "1. Make all requests in a loop, producing a list of {'locId': 'earthdata_taskID'}\n",
    "2. For each locId, pull each .csv into a dataframe then load that dataframe into a database table bearing the name that corresponds with the layer and product. Append the locId.\n",
    "    EXAMPLE: the contents of the .csv file for 'MCD12Q1-006-LC-Prop1-Statistics.csv' go into the table 'MCD12Q1-006-LC-Prop1-Statistics' in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##small frame of hotspots for testing\n",
    "hotspotsGeo_short = hotspotsGeo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #44 resolve error: {'message': \"RecursionError('maximum recursion depth exceeded in __instancecheck__',)\"} --DONE\n",
    "##TODO #45 once all hotspots/locId's have Prop1 and Type3 MCD12Q1 data, convert this block to check for LAI and fPAR\n",
    "earthdataTaskList = []\n",
    "earthdata_errorLog = []\n",
    "for locId in hotspotsGeo_short.itertuples():\n",
    "    ##time.sleep(5.0)\n",
    "    ##package polygon as a geoJSON\n",
    "    yy = locId.locId\n",
    "    glhf = {\"type\": \"FeatureCollection\", \"features\":\n",
    "        [{\n",
    "        \"type\":\"Feature\",\n",
    "            \"geometry\":\n",
    "                {\"type\": \"Polygon\",\n",
    "                \"coordinates\":\n",
    "                    '[[locId.NW,locId.NE,locId.SE,locId.SW,locId.NW]]'\n",
    "                },\n",
    "            \"properties\": {}}]\n",
    "        }\n",
    "    glhf_json = geojson.GeoJSON(glhf)\n",
    "    ##compile JSON task request\n",
    "    earthdata_task = {\n",
    "        'task_type': 'area',\n",
    "        'task_name': yy,\n",
    "        'params': {\n",
    "            'dates': \n",
    "                [{\"endDate\": \"12-31\", \n",
    "                \"recurring\": True, \n",
    "                \"startDate\": \"01-01\", \n",
    "                \"yearRange\": [2017, 2019]}],\n",
    "            'layers': \n",
    "                [{\"layer\": \"LC_Prop1\", \"product\": \"MCD12Q1.006\"}, \n",
    "                {\"layer\": \"LC_Type3\", \"product\": \"MCD12Q1.006\"}],\n",
    "            'output': {\n",
    "                'format': {\n",
    "                    'type': 'netcdf4'}, \n",
    "                    'projection': 'geographic'},\n",
    "        'geo':glhf_json}}\n",
    "    ##submit the task request\n",
    "    \n",
    "##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log --DONE\n",
    "earthdata_taskReq = requests.post('{}task'.format(earthdata_baseUrl),json=earthdata_task,headers=earthdata_head)\n",
    "##TODO #47 an index is needed in the line below due to scalar values; figure out how to pass in an incremeneting index or find some other solve\n",
    "##start over with lists here\n",
    "ED_taskResp = pd.DataFrame(earthdata_taskReq.json(),index=[25])\n",
    "\n",
    "##TODO #46 something about this is broken, fix it\n",
    "if earthdata_taskReq.status_code == 202:\n",
    "    ##ED_taskResp = pd.DataFrame(earthdata_taskReq.json())\n",
    "    ED_taskResp.reset_index(inplace=True)\n",
    "    earthdataTaskList.append(ED_taskResp)\n",
    "else: \n",
    "    ##ED_taskResp = pd.DataFrame(earthdata_taskReq.json())\n",
    "    ED_taskResp.reset_index(inplace=True)\n",
    "    ED_taskResp['statusCode'] = earthdata_taskReq.status_code\n",
    "    ED_taskResp['request'] = json.dumps(earthdata_task)\n",
    "    ED_taskResp['reqDate'] = dt.datetime.today()\n",
    "    earthdata_errorLog.append(ED_taskResp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##compile either the error list or continue with the tasks\n",
    "\n",
    "EDTaskList = pd.DataFrame()\n",
    "try:\n",
    "    EDTaskList = pd.concat(earthdataTaskList)\n",
    "except ValueError: \n",
    "    try:\n",
    "        earthdata_errorLog = pd.concat(earthdata_errorLog)\n",
    "        earthdata_errorLog.to_sql('earthdata_errorlog',con=cnx,if_exists='append')\n",
    "    except ValueError: raise Exception\n",
    "else: pd.concat(earthdataTaskList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##submit the task request\n",
    "##TODO #42 if the request returns anything other than a task_id, stop the script and write the returned issue to the error log\n",
    "params = {'limit': 15, 'pretty': True}\n",
    "earthdata_tasks = requests.get('{}task'.format(earthdata_baseUrl),params=params,headers=earthdata_head).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id in earthdataTaskList.itertuples():\n",
    "##TODO #39 check status of the appEEARS task request --DONE\n",
    "import time\n",
    "earthdata_tasksResp = requests.get('{}task/{}'.format(earthdata_baseUrl,earthdata_taskID),headers=earthdata_head).json()\n",
    "starttime = time.time()\n",
    "while earthdata_tasksResp['status'] !='done':\n",
    "    print(requests.get('{}task/{}'.format(earthdata_baseUrl,earthdata_taskID),headers=earthdata_head).json()['status'])\n",
    "    time.sleep(20.0 - ((time.time() - starttime) % 20.0))\n",
    "print(requests.get('{}task/{}'.format(earthdata_baseUrl,earthdata_taskID),headers=earthdata_head).json()['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO #38 get the results of the appEEARS task as a bundle\n",
    "earthdata_bundle = requests.get('{}bundle/{}'.format(earthdata_baseUrl,earthdata_taskID),).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##filter for .csv's\n",
    "earthdata_files = {}\n",
    "for f in earthdata_bundle['files']:\n",
    "    if f['file_type'] in 'csv':\n",
    "        earthdata_files[f['file_id']] = f['file_name']\n",
    "    else: continue\n",
    "\n",
    "earthdata_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##??\n",
    "for file_id in earthdata_files:\n",
    "    allresults = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IT WORKS\n",
    "##To read results to a dataframe, pass in earthdata_baseUrl + 'bundle/ + taskID/ + file_id/ + file_name\n",
    "##example: https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv\n",
    "qrf = pd.DataFrame()\n",
    "qrf = pd.read_csv('https://lpdaacsvc.cr.usgs.gov/appeears/api/bundle/098dbb7a-0dfc-4f19-8410-a1db4f91170c/5a51d0f8-362a-4ea3-9e8e-ce1bd783aa62/MCD12Q1-006-LC-Prop1-Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##close the connection\n",
    "cnx.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
